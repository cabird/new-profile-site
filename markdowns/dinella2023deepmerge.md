# DeepMerge: Learning to Merge Programs

**Abstract**

In collaborative software development, program merging is the mechanism to integrate changes from multiple programmers. Merge algorithms in modern version control systems report a conflict when changes interfere textually. Merge conflicts require manual intervention and frequently stall modern continuous integration pipelines. Prior work found that, although costly, a large majority of resolutions involve rearranging text without writing any new code. Inspired by this observation we propose the first data-driven approach to resolve merge conflicts with a machine learning model. We realize our approach in a tool DEEPMERGE that uses a novel combination of (i) an edit-aware embedding of merge inputs and (ii) a variation of pointer networks, to construct resolutions from input segments. We also propose an algorithm to localize manual resolutions in a resolved file and employ it to curate a ground-truth dataset comprising 8,719 non-trivial resolutions in JavaScript programs.Our evaluation shows that, on a held out test set, DEEPMERGE can predict correct resolutions for 37% of non-trivial merges, compared to only 4% by a state-of-the-art semistructured merge technique. Furthermore, on the subset of merges with upto 3 lines (comprising 24% of the total dataset), DEEPMERGE can predict correct resolutions with 78% accuracy.

## INTRODUCTION

In collaborative software development settings, version control systems such as "git" are commonplace. Such version control systems allow developers to simultaneously edit code through features called branches. Branches are a growing trend in version control as they allow developers to work in their own isolated workspace, making changes independently, and only integrating their work into the main line of development when it is complete. Integrating these changes frequently involves merging multiple copies of the source code. In fact, according to a large-scale empirical study of Java projects on GitHub (Ghiotto, "On the nature of merge conflicts: A study of 2, 731 open source java projects hosted by github"), nearly 12% of all commits are related to a merge.
To integrate changes by multiple developers across branches, version control systems utilize merge algorithms. Textual three-way file merge (e.g. present in "git merge") is the prevailing merge algorithm. As the name suggests, threeway merge takes three files as input: the common base file O, and its corresponding modified files, A and B. The algorithm either:
1) declares a "conflict" if the two changes interfere with each other, or 2) provides a merged file M that incorporates changes made in A and B. Under the hood, three-way merge typically employs the diff3 algorithm, which performs an unstructured (linebased) merge (Smith, "Gnu diff3. distributed with GNU diffutils package"). Intuitively, the algorithm aligns the twoway diffs of A (resp. B) over the common base O into a sequence of diff slots. At each slot, a change from either A or B is incorporated. If both programs change a common slot, a merge conflict is produced, and requires manual resolution of the conflicting modifications.
Figure 1 shows two simple code snippets to illustrate examples of three-way merge inputs and outputs. The figure shows the base program file O along with the two variants A and B. Example (Alon, "code2seq: Generating sequences from structured representations of code") shows a case where diff3 successfully provides a merged file M incorporating changes made in both A and B. On the other hand, Example (Apel, "Structured merge with auto-tuning: balancing precision and performance") shows a case where diff3 declares a conflict because two independent changes (updates to x and z) occur in the same diff slot.
When diff3 declares a conflict, a developer must intervene. Consequently, merge conflicts are consistently ranked as one of the most taxing issues in collaborative, open-source software development, "especially for seemingly less experienced developers" (Gousios, "Work practices and challenges in pull-based development: the contributor's perspective"). Merge conflicts impact developer productivity, resulting in costly broken builds that stall the continuous integration (CI) pipelines for several hours to days. The fraction of merge conflicts as a percentage of merges range from 10% -20% for most collaborative projects. In several large projects, merge conflicts account for up to 50% of merges (see (Ghiotto, "On the nature of merge conflicts: A study of 2, 731 open source java projects hosted by github") for details of prior studies).
Merge conflicts often arise due to the unstructured diff3 algorithm that simply checks if two changes occur in the same diff slot. For instance, the changes in Example (2), although textually conflicting, do not interfere semantically. This insight has inspired research to incorporate program structure and semantics while performing a merge. Structured merge approaches (Apel, "Semistructured merge: Rethinking merge in revision control systems"), (Leßenich, "Balancing precision and performance in structured merge"), (Tavares, "Semistructured Merge in JavaScript Systems") and their variants treat merge inputs as abstract syntax trees (ASTs), and use treestructured merge algorithms. However, such approaches still yield a conflict on merges such as Example (2) above, as they do not model program semantics and cannot safely reorder statements that have side effects. 1 To make matters worse, the gains from structured approaches hardly transfer to dynamic languages, namely JavaScript (Tavares, "Semistructured Merge in JavaScript Systems"), due to the absence of static types. Semantics-based approaches (Yang, "A program integration algorithm that accommodates semantics-preserving transformations"), (Sousa, "Verified three-way program merge") can, in theory, employ program analysis and verifiers to detect and synthesize the resolutions. However, there are no semantics-based tools for synthesizing merges for any real-world programming language, reflecting the intractable nature of the problem. Current automatic approaches fall short, suggesting that merge conflict resolution is a nontrivial problem.
This paper takes a fresh data-driven approach to the problem of resolving unstructured merge conflicts. Inspired by the abundance of data in open-source projects, the paper demonstrates how to collect a dataset of merge conflicts and resolutions.
This dataset drives the paper's key insight: a vast majority (80%) of resolutions do not introduce new lines. Instead, they consist of (potentially rearranged) lines from the conflicting region. This observation is confirmed by a prior independent large-scale study of Java projects from GitHub (Gousios, "Work practices and challenges in pull-based development: the contributor's perspective"), in which 87% of resolutions are comprised exclusively from lines in the input. In other words, a typical resolution consists of re-arranging conflicting lines without writing any new code. Our observation naturally begs the question: Are there latent patterns of rearrangement? Can these patterns be learned?
This paper investigates the potential for learning latent patterns of rearrangement. Effectively, this boils down to the question:
Can we learn to synthesize merge conflict resolutions? Specifically, the paper frames merging as a sequence-tosequence task akin to machine translation.
To formulate program merging as a sequence-to-sequence problem, the paper considers the text of programs A, B, and O as the input sequence, and the text of the resolved program M as the output sequence. However, this seemingly simple formulation does not come without challenges. Section 5 demonstrates an out of the box sequence-to-sequence model trained on merge conflicts yields very low accuracy. In order to effectively learn a merge algorithm, one must: 1) represent merge inputs in a concise yet sufficiently expressive sequence; 2) create a mechanism to output tokens at the line granularity; and
1. We ran jdime (Leßenich, "Balancing precision and performance in structured merge") in structured mode on this example after translating the code snippet to Java. 3) localize the merge conflicts and the resolutions in a given file.
To represent the input in a concise yet expressive embedding, the paper shows how to construct an edit aware sequence to be consumed by DEEPMERGE. These edits are provided in the format of diff3 which is depicted in Figure 2(a) in the portion between markers "<<<<<<<" and ">>>>>>>". The input embedding is extracted from parsing the conflicting markers and represents A's and B's edits over the common base O.
To represent the output at the line granularity, DEEP-MERGE's design is a form of a pointer network (Vinyals, "Pointer networks"). As such, DEEPMERGE constructs resolutions by copying input lines, rather than learning to generate them token by token. Guided by our key insight that a large majority of resolutions are entirely comprised of lines from the input, such an output vocabulary is sufficiently expressive.
Lastly, the paper shows how to localize merge conflicts and the corresponding user resolutions in a given file. This is necessary as our approach exclusively aims to resolve locations in which diff3 has declared a conflict. As such, our algorithm only needs to generate the conflict resolution and not the entire merged file. Thus, to extract ground truth, we must localize the resolution for a given conflict in a resolved file. Localizing such a resolution region unambiguously is a non-trivial task. The presence of extraneous changes unrelated to conflict resolution makes resolution localization challenging. The paper presents the first algorithm to localize the resolution region for a conflict. This ground truth is essential for training such a deep learning model.
The paper demonstrates an instance of DEEPMERGE trained to resolve unstructured merge conflicts in JavaScript programs. Besides its popularity, JavaScript is notorious for its rich dynamic features, and lacks tooling support. Existing structured approaches struggle with JavaScript (Tavares, "Semistructured Merge in JavaScript Systems"), providing a strong motivation for a technique suitable for dynamic languages. The paper contributes a real-world dataset of 8,719 merge tuples that require non-trivial resolutions from nearly twenty thousand repositories in GitHub. Our evaluation shows that, on a held out test set, DEEPMERGE can predict correct resolutions for 37% of non-trivial merges. DEEPMERGE's accuracy is a 9x improvement over a recent semistructured approach (Tavares, "Semistructured Merge in JavaScript Systems"), evaluated on the same dataset. Furthermore, on the subset of merges with upto 3 lines (comprising 24% of the total dataset), DEEPMERGE can predict correct resolutions with 78% accuracy.
Contributions. In summary, this paper:
1) is the first to define merge conflict resolution as a machine learning problem and identify a set of challenges for encoding it as a sequence-to-sequence supervised learning problem ( § 2). 2) presents a data-driven merge tool DEEPMERGE that uses edit-aware embedding to represent merge inputs and a variation of pointer networks to construct the resolved program ( § 3). 3) derives a real-world merge datasetfor supervised learning by proposing an algorithm for localizing resolution regions ( § 4). 4) performs an extensive evaluation of DEEPMERGE on merge conflicts in real-world JavaScript programs. And, demonstrates that it can correctly resolve a significant fraction of unstructured merge conflicts with high precision and 9x higher accuracy than a structured approach.

## DATA-DRIVEN MERGE

We formulate program merging as a sequence-to-sequence supervised learning problem and discuss the challenges we must address in solving the resulting formulation.

## Problem Formulation

A merge consists of a 4-tuple of programs (A, B, O, M) where A and B are both derived from a common O, and M is the developer resolved program.
A merge may consist of one or more regions. We define a merge tuple ((A, B, O), R) such that A, B, O are (sub) programs that correspond to regions in A, B, and O, respectively, and R denotes the result of merging those regions. Although we refer to (A, B, O, R) as a merge tuple, we assume that the tuples also implicitly contain the programs that they came from as additional contexts (namely A, B, O, M). Definition 1 (Data-driven Merge). Given a dataset of M merge tuples,
a data-driven merge algorithm merge is a function that maximizes:
treating Boolean outcomes of the equality comparison as integer constants 1 (for true) and 0 (for false).
In other words, merge aims to maximize the number of merges from D. Rather than constraining merge to exactly satisfy all merge tuples in D, we relax the objective to maximization. A perfectly satisfying merge function may not exist in the presence of a real-world noisy dataset D. For instance, there may be
In other words, two merge tuples consist of the same edits but different resolutions. Example 1. Figure 3(a) shows a merge instance that we will use as our running example throughout. This instance is formulated in our setting as the merge tuple (A, B, O, R) depicted in Figure 3 the third line of A. For this example, the R also incorporates the intents from both A and B intuitively, assuming b does not appear in the rest of the programs. One possible way to learn a merge algorithm is by modeling the conditional probability
In other words, a model that generates the output program R given the three input programs.
Because programs are sequences, we further decompose Eq 1 by applying the chain rule (Sutskever, "Sequence to sequence learning with neural networks"):
This models the probability of generating the j-th element of the program, given the elements generated so far. There are many possible ways to model a three-way merge. However, the above formulation suggests one obvious approach is to use a maximum likelihood estimate of a sequence-tosequence model.

## Challenges

Applying a sequence-to-sequence (Seq2seq) model to merge conflict resolution poses unique challenges. We discuss three key challenges, concerning input representation, output construction, and dataset extraction.

## Representing the Merge Inputs as a Sequence.

In a traditional sequence-to-sequence task such as machine translation, there is a single input sequence that maps to a single output sequence. However, in our case, we have three input sequences of varying sizes, corresponding to the three versions of a program involved in a merge conflict. It is not immediately evident how to determine a suitable token granularity and encode these sequences in a manner that is amenable to learning. One obvious solution is to concatenate the tokens of the three sequences to obtain a single sequence. However, the order of concatenation is unclear. Furthermore, as we show in Section 3.2, such a naive representation not only suffers from information loss and truncation, but also poor precision by being unaware of A and B's edits over common base O. In summary, we have: CH1: Encode programs A, B, and O as the input to a Seq2Seq model.

## Constructing the Output Resolution

Our key insight that a majority of resolutions do not introduce new lines leads us to construct the output resolution directly from lines in the conflicting region. This naturally suggests the use of pointer networks (Vinyals, "Pointer networks"), an encoder-decoder architecture capable of producing outputs explicitly pointing to tokens in the input sequence. However, a pointer network formulation suggests an equivalent input and output granularity. In Section 3.2, we show that the input is best represented at a granularity far smaller than lines.
Thus, the challenge is: CH2: Output R at the line granularity given a non-line granularity input.

## Extracting Ground Truth from Raw Merge Data.

Finally, to learn a data-driven merge algorithm, we need real-world data that serves as ground truth. Creating this dataset poses non-trivial challenges. First, we need to localize the resolution region and corresponding conflicting region.
In some cases, developers performing a manual merge resolution made changes unrelated to the merge. Localizing resolution regions unambiguously from input programs is challenging due to the presence of these unrelated changes. Second, we need to be able to recognize and subsequently filter merge resolutions that do not incorporate both the changes. In summary, we have:
given (A, B, O, M).

## THE DEEPMERGE ARCHITECTURE

Section 2 suggested one way to learn a three-way merge is through a maximum likelihood estimate of a sequence-tosequence model. In this section we describe DEEPMERGE, the first data-driven merge framework, and discuss how it addresses challenges CH1 and CH2. We motivate the design of DEEPMERGE by comparing it to a standard sequence-tosequence model, the encoder-decoder architecture.

## Encoder Decoder Architectures

Sequence-to-sequence models aim to map a fixed-length input ((X N ) N ∈N ), to a fixed-length output, ((Y M ) M ∈N ). 2 The standard sequence-to-sequence model consists of three components: an input embedding, an encoder, and a decoder. Input embedding: An embedding maps a discrete input from an input vocabulary V (x n ∈ N |V | ), to a continuous D dimensional vector space representation (x n ∈ R D ). Such a mapping is obtained by multiplication over an embedding matrix E ∈ R D×|V | . Applying this for each element of X N gives X N .
Encoder: An encoder encode, processes each x n and produces a hidden state, z n which summarizes the sequence upto the n-th element. At each iteration, the encoder 2. Note that M is not necessary equal to N .
takes as input the current sequence element x n , and the previous hidden state z n-1 . After processing the entire input sequence, the final hidden state, z N , is passed to the decoder.
Decoder: A decoder decode, produces the output sequence Y M from an encoder hidden state Z n . Similar to encoders, decoders work in an iterative fashion. At each iteration, the decoder produces a single output token y m along with a hidden summarization state h m . The current hidden state and the previous predicted token y m are then used in the following iteration to produce y m+1 and h m+1 . Each y m the model predicts is selected through a softmax over the hidden state:
DEEPMERGE is based on this encoder-decoder architecture with two significant differences.
First, rather than a standard embedding followed by encoder, we introduce a novel embedding method called Merge2Matrix. Merge2Matrix addresses CH1 by summarizing input programs (A, B, O) into a single embedding fed to the encoder. We discuss our Merge2Matrix solution as well as less effective alternatives in Section 3.2.
Second, rather than using a standard decoder to generate output tokens in some output token vocabulary, we augment the decoder to function as a variant of pointer networks. The decoder outputs line tuples (i, W ) where W ∈ {A, B} and i is the i-th line in W . We discuss this in detail in Section 3.4.
Example 2. Figure 4 illustrates the flow of DEEPMERGE as it processes the inputs of a merge tuple. First, the raw text of A, B, and O is fed to Merge2Matrix. As the name suggests, Merge2Matrix summarizes the tokenized inputs as a matrix. That matrix is then fed to an encoder which computes the encoder hidden state z N . Along with the start token for the decoder hidden state, the decoder takes z N and iteratively (denoted by the • • • ) generates as output the lines to copy from A and B. The final resolution is shown in the green box.

## Merge2Matrix

An encoder takes a single sequence as input. As discussed in Section 2.2, a merge tuple consists of three sequences. This section introduces Merge2Matrix, an input representation that expresses the tuple as a single sequence. It consists of embedding, transformations to summarize embeddings, and finally, edit-aware alignment.

## Tokenization and Embedding

This section discusses our relatively straightforward application of both tokenization and embedding.
Tokenization. Working with textual data requires tokenization whereby we split a sequence of text into smaller units referred to as tokens. Tokens can be defined at varying granularities such as characters, words, or sub-words. These units form a vocabulary which maps input tokens to integer indices. Thus, a vocabulary is a mapping from a sequence of text to a sequence of integers. This paper uses byte-pair encoding (BPE) as it has been shown to work well with source code, where tokens can be formed by combining different words via casing conventions (e.g. snake_case or
Encoder line <1, A> STOP token Decoder <1, B> line <1, O> line <2, O> line <1, B> line <2, A> line <3, A> START token line <1, A> STOP token Decoder line <1, O> line <2, O> line <1, B> line <2, A> line <3, A> Decoder <3, A> hidden state hidden state hidden state <<<<<<< A let b = x + 5.7 <1, A> var y = floor(b) <2, A> console.log(y) <3, A> ||||||| var b = 5.7 <1, O> var y = floor(b) <2, O> ======= O var y = floor(x + 5.7) <1, B> >>>>>>> B Merge2Matrix Encoder line <1,A> STOP token line <1,O> line <2,O> line <1,B> line <2,A> line <3,A> Z n h m <<<<<<< A let b = x + 5.7 <1,A> var y = floor(b) <2,A> console.log(y) <3,A> ||||||| var b = 5.7 <1,O> var y = floor(b) <2,O> ======= O var y = floor(x + 5.7) <1,B> >>>>>>> B Merge2Matrix Decoder <1,B> h M line <1,A> STOP token line <1,O> line <2,O> line <1,B> line <2,A> line <3,A> var y = floor(x + 5.7) <1,B> console.log(y) <3,A> Encoder line <1,A> STOP token line <1,O> line <2,O> line <1,B> line <2,A> line <3,A> START token <3,A> Z Z <<<<<<< A let b = x + 5.7 <1,A> var y = floor(b) <2,A> console.log(y) <3,A> ||||||| var b = 5.7 <1,O> var y = floor(b) <2,O> ======= O var y = floor(x + 5.7) <1,B> >>>>>>> B Merge2Matrix Decoder <1,B> line <1,A> STOP token line <1,O> line <2,O> line <1,B> line <2,A> line <3,A> Z Decoder Decoder Z line <1,A> STOP token line <1,O> line <2,O> line <1,B> line <2,A> line <3,A> var y = floor(x + 5.7) <1,B> console.log(y) <3,A> Decoder Decoder START token ... camelCase) causing a blowup in vocabulary size (Karampatsis, "Big code != big vocabulary: Open-vocabulary models for source code"). Bytepair encoding is an unsupervised sub-word tokenization that draws inspiration from information theory and data compression wherein frequently occurring sub-word pairs are recursively merged and stored in the vocabulary. We found that the performance of BPE was empirically superior to other tokenization schemes.
Embedding. Given an input sequence X N , and a hyperparameter (embedding dimension) D, an embedding transformation creates X N . As described in Section 3.1, the output of this embedding is then fed to an encoder. Because a merge tuple consists of three inputs (A, B, and O), the following sections introduce novel transformations that summarize these three inputs into a format suitable for the encoder.

## Merge Tuple Summarization

In this section, we describe summarization techniques that are employed after embedding. Before we delve into details, we first introduce two functions used in summarization.
Suppose a function that concatenates embedded representations:
that takes s similarly shaped tensors as arguments and concatenates them along their last dimension. Concatenating these s embeddings increases the size of the encoder's input by a factor of s.
Suppose a function linearize that linearly combines s embedded representations. We parameterize this function with learnable parameters θ ∈ R s+1 . As input, linearize takes an embedding x i ∈ R D for i ∈ 1..S. Thus, we define
where all operations on the inputs x 1 , . . . , x s are pointwise. linearize reduces the size of the embeddings fed to the encoder by a factor of s. Now that we have defined two helper functions, we describe two summarization methods.
Naïve. Given a merge tuple's inputs (A, B, O), a naïve implementation of Merge2Matrix is to simply concatenate the embedded representations (i.e., concat 3 (A, B, O)) Traditional sequence-to-sequence models often suffer from information forgetting; as the input grows longer, it becomes harder for encode to capture long-range correlations in that input. A solution that addresses CH1, must be concise while retaining the information in the input programs.
Linearized. As an attempt at a more concise representation, we introduce a summarization we call linearized. This method linearly combines each of the embeddings through our helper function: linearize θ (A, B, O). In Section 5 we empirically demonstrate better model accuracy when we summarize with linearize θ rather than concat s .

## Edit-Aware Alignment

In addition to input length, CH1 also alludes that an effective input representation needs to be "edit aware". The aforementioned representations do not provide any indication that A and B are edits from O.
Prior work, Learning to Represent Edits (LTRE) (Yin, "Learning to represent edits") introduces a representation to succinctly encode 2 two-way diffs. The method uses a standard deterministic diffing algorithm and represents the resulting pair-wise alignment as an autoencoded fixed dimension vector.
A two-way alignment produces an "edit sequence". This series of edits, if applied to the second sequence, would produce the first. An edit sequence, ∆ AO , is comprised of the following editing actions: = representing equivalent tokens, + representing insertions,representing deletions, ↔ representing a replacement. Two special tokens ∅ and | are used as a padding token and a newline marker, respectively. Note that these ∆s only capture information about the kinds of edits and ignore the the tokens that make up the edit itself (with the exception of the newline token). Prior to the creation of ∆, a preprocessing step adds padding tokens such that equivalent tokens in A (resp. B) and O are in the same position. These sequences, shown in Figure 5 are denoted as A and AO (resp. B and BO ).
Example 3. Consider B's edit to O in Figure 5 via its preprocessed sequences B , BO , and its edit sequence ∆ BO . One intuitive view of ∆ BO is that it is a set of instructions that describe how to turn B into BO with the aforementioned semantics. Note the padding token ∅ introduced into ∆ BO represents padding out to the length of the longer edit sequence ∆ AO .
We now describe two edit-aware summarization methods based on this edit-aware representation. However, our setting differs from the original LTRE setting as we assume three input sequences and a three-way diff. In the following summarization methods, we assume that A, B, O are tokenized, but not embedded before invoking Merge2Matrix. Aligned naïve. Given ∆ AO and ∆ BO , we embed each to produce ∆ AO and ∆ BO , respectively. Then we combine these embeddings through concatenation and thus concat 2 (∆ AO , ∆ BO ) is fed to the encoder.
Aligned linearized. This summarization method is depicted in Figure 5, invoking linearize to construct an input representation over edit sequences. First, we apply alignment to create ∆ AO and ∆ BO . This is portrayed through the operator. Following construction of the ∆s, we apply embedding and subsequently apply our edit-aware linearize operation via the operator. Thus, we summarize embeddings with linearize θ (∆ AO , ∆ BO ) and feed its output to the encoder. As we demonstrate in Section 5, this edit-aware input representation significantly increases the model's accuracy.
LTRE. Finally, for completeness, we also include the original LTRE representation. We modify this to our setting by creating two 2-way diffs. The original LTRE has a second key difference from our summarization methods. LTRE includes all tokens from from the input sequences in addition to the edit sequences That is, LTRE summarizes A AO , ∆ AO , B , BO , and ∆ BO . Let A , AO and ∆ AO , (resp B , BO , and ∆ BO ) be the embedding of a two-way diff. Then, the following summarization combines all embeddings: concat 6 (∆ AO , A , AO , ∆ BO , B , BO )

## The Encoder

The prior sections described Merge2Matrix which embeds a merge into a continuous space which is then summarized by an encoder. DEEPMERGE uses a bi-directional gated recurrent unit (Cho, "Learning phrase representations using rnn encoder-decoder for statistical machine translation") (GRU) to summarize the embedded input sequence. We empirically found that a bi-directional GRU was more effective than a uni-directional GRU.

## Synthesizing Merge Resolutions

This section summarizes DEEPMERGE's approach to solving CH2. Given a sequence of hidden vectors Z N produced by an encoder, a decoder generates output sequence Y M . We introduce an extension of a traditional decoder to copy lines of code from those input programs.
Denote the number of lines in A and B as Li A and Li B , respectively. Suppose that L = 1..(Li A + Li B ); then, a value i ∈ L corresponds to the i-th line from A if i <= Li A , and the i -Li A -th line from B, otherwise.
Given merge inputs (A, B, O), DEEPMERGE's decoder computes a sequence of hidden states H M , and models the conditional probability of lines copied from the input programs A, B, and O by predicting a value in y m ∈ Y M :
where h m ∈ H M is the decoder hidden state at the m-th element of the output sequence and the argmax(y m ) yields an index into L.
In practice, we add an additional STOP token to L. The STOP token signifies that the decoder has completed the sequence. The STOP token is necessary as the decoder may output a variable number of lines conditioned on the inputs. This formulation is inspired by pointer networks (Vinyals, "Pointer networks"), an encoder-decoder architecture that outputs an index that explicitly points to an input token. Such networks are designed to solve combinatorial problems like sorting. Because the size of the output varies as a function of the input, a pointer network requires a novel attention mechanism that applies attention weights directly to the input sequence. This differs from traditional attention networks which are applied to the outputs of the encoder Z N . In contrast, DEEPMERGE requires no change to attention. Our architecture outputs an index that points to the abstract concept of a line, rather than an explicit token in the input. Thus, attention applied to Z N , a summarization of the input, is sufficient.

## Training and Inference with DEEPMERGE

The prior sections discussed the overall model architecture of DEEPMERGE. This section describes hyperparameters that control model size and how we trained the model. We use a embedding dimension D = 1024 and 1024 hidden units in the single layer GRU encoder. Assume the model parameters are contained in θ; training seeks to find the values of θ that maximize the log-likelihood arg max θ log p θ (R|A, B, O) over all merge tuples ((A, B, O), R) in its training dataset. We use standard cross-entropy loss with the Adam optimizer. Training takes roughly 18 hours on a NVIDIA P100 GPU and we pick the model with the highest validation accuracy, which occurred after 29 epochs.
Finally, during inference time, we augment DEEPMERGE to use standard beam search methods during decoding to produce the most likely k top merge resolutions. DEEP-MERGE predicts merge resolutions up to C lines. We set C = 30 to tackle implementation constraints and because most resolutions are less than 30 lines long. However, we evaluate DEEPMERGE on a full test dataset including samples where the number of lines in M is ≥ C.

## REAL-WORLD LABELED DATASET

This section describes our solution to CH3: localizing merge instances (A, B, O, R) i from (A, B, O, M). Since a program may have several merge conflicts, we decompose the overall merge problem into merging individual instances. As shown in Figure 3, A, B, and O regions can be easily extracted given the diff3 conflict markers. However, reliably localizing a resolution R involves two sub-challenges: 1) How do we localize individual regions R unambiguously? 2) How do we deal with trivial resolutions? In this section, we elaborate on each of these sub-challenges and discuss our solutions. We conclude with a discussion of our final dataset and its characteristics.
Algorithm 1 denotes a method to localize merge tuples from a corpus of merge conflict and resolution files. The top-level procedure EXTRACTMERGETUPLES takes C, the diff3 conflict file with markers, along with M, the resolved file. From those inputs, it extracts merge tuples into MT . The algorithm loops over each of the conflicted regions in C, and identifies the input (A, B, O) and output (R) of the tuple using GETCONFLICTCOMPONENTS and LOCALIZERESREGION respectively. Finally, it applies a filter on the extracted tuple (lines 5 -14). We explain each of these components in the next few subsections.

## Localization of Resolution Regions

Creating a real-world merge conflict labeled dataset requires identifying the "exact" code region that constitutes a resolution. However, doing so can be challenging; Figure 6 demonstrates an example. The developer chooses to perform a resolution baz(); that does not correspond to anything from the A or B edits, and the surrounding context also undergoes changes (e.g. changing var with let which restricts Algorithm 1 Localizing Merge Tuples from Files for Dataset
1: procedure LOCALIZEMERGETUPLES(C, M) 2: MT ← ∅ Merge Tuples 3: for i ∈ [1, NUMCONFLICTS(C)] do 4: R ← LOCALIZERESREGION(C, M, i) 5: if R == nil then 6: continue Could not find resolution 7: end if 8: (A, B, O) ←GETCONFLICTCOMPONENTS(C, i) 9: if R ∈ {A, B, O} then 10: continue Filter trivial resolutions 11: end if 12: if LINES(R) ⊆ LINES(A) ∪ LINES(B) then 13: MT ← MT ∪ {(A, B , O, R)} 14: end if 15: end for 16: return MT 17: end procedure 18: procedure LOCALIZERESREGION(C, M, i) 19: n ← Length(M) Length of M in chars 20: m ← Length(C) Length of C in chars 21: (spos, epos) ← GETCONFLICTSTARTEND(C, i) 22: prfx ← BOF + C[0 : spos] 23: sffx ← C[epos : m] + EOF 24: s ← MINIMALUNIQUEPREFIX(reverse(prfx ), reverse(M)) 25: e ← MINIMALUNIQUEPREFIX(sffx , M) 26: if s ≥ 0 and e ≥ 0 then 27: return M[n -s : e] 28: else 29: return nil 30: end if 31: end procedure 32: procedure MINIMALUNIQUEPREFIX(x, y) 33: Output:Returns the start position of the minimal non-empty prefix of x that appears uniquely in y, else -1 34: end procedure 35: procedure LINES(p) 36: Output:Returns the set of lines comprising the program p 37: end procedure <BOF> ... var time = new Date(); print_time(time); <<<<<<< a.js x = foo(); ||||||| base.js ======= x = bar(); >>>>>>> b.js print_time(time); <EOF>
(a) A merge instance. <BOF> ... let time = new Date(); print_time(time); baz(); print_time(time); <EOF> (b) Resolution. the scope in the prefix). To the best of our knowledge, there is no known algorithm to localize R for such cases.
LOCALIZERESREGION is our method that tries to localize the i th resolution region R, or returns nil when unsuccessful. Intuitively, we find a prefix and suffix in a merge instance and use this prefix and suffix to bookend a resolution. If we cannot uniquely find those bookends, we say the resolution is ambiguous.
The method first obtains the prefix prfx (resp. suffix sffx ) of the i th conflict region in C in line 22 (resp. line 23). We add the start of file BOF and end of file EOF tokens to the prefix and suffix respectively. The next few lines try to match the prefix prfx (resp. suffix sffx ) in the resolved file M unambiguously. Let us first focus on finding the suffix of the resolution region in M in line 25. The procedure MINIMALUNIQUEPREFIX takes two strings x and y and finds the start position of the minimal non-empty prefix of x that appears uniquely in y, or returns -1. For example, MINIMALUNIQUEPREFIX("abc", "acdabacc") is 3 since "ab" is the minimal prefix of x that appears uniquely in y starting in position 3 (0-based indexing).
To find the prefix of the resolution, we reverse the prfx string and search for matches in reversed M, and then finally find the offset from the start of M by subtracting s from the length n of M. The unique occurrence of both the prefix and suffix in M allows us to map the conflicted region to the resolved region.
For our example, even though the line "print_time(time);" that encloses the conflicted region appears twice in M, extending it by "time = new Date();" in the prefix and EOF in the suffix provides a unique match in M. Thus, the algorithm successfully localizes the desired region "baz();" as the resolution region.
After localizing the resolution regions, we have a set of merge instances of the form (A, B, O, R). We can use our definition from Section 2 to label a merge tuple (A, B, O, R).

## Filtering Trivial Resolutions

Upon examining our dataset, we found a large set of merges in which A was taken as the resolution and B was entirely ignored (or vice versa). These trivial samples, in large, were the product of running git merge with "ours" or "theirs" command-line options. Using these merge options indicates that the developer did not resolve the conflict after careful consideration of both branches, but instead relied on the git interface to completely drop one set of changes. The aforementioned command-line merge options are typically used the commit is the first of many fix-up commits to perform the full resolution.
We appeal to the notion of a "valid merge" that tries to incorporate both the syntactic and semantic changes from both A and B. Thus, these samples are not valid as they disregard the changes from B (resp. A) entirely. Furthermore, these trivial samples comprised 70% of our "pre-filtering" dataset. Previous work confirmed our observation that a majority of merge resolutions in GitHub Java projects (75% in Table 13 (Ghiotto, "On the nature of merge conflicts: A study of 2, 731 open source java projects hosted by github")) correspond to taking just A or B. To avoid polluting our dataset, we filter such merges (A, B, O, R) where R ∈ {A, B, O} (line 9 in Algorithm 1). Our motivation to filter the dataset of trivial labels is based on both dataset bias and the notion of a valid merge.

## Final Dataset

We crawled repositories in GitHub containing primarily JavaScript files, looking at merge commits. To avoid noise and bias, we select projects that were active in the past one year (at the time of writing), and received at least 100 stars (positive sentiment). We also verified that the dataset did not contain duplicate merges. We ignore minified JavaScript files that compress an entire JavaScript file to a A and B, and (iii) preserve the order of lines within A and B. These heuristic restrictions are based on manual observations that a large fraction of resolutions satisfy these conditions. Table 1 shows SCANMERGE's performance averaged over 10 trials. DEEPMERGE performs significantly better in terms of top-1 resolution accuracy (36.50% vs 4.20%). SCANMERGE only synthesizes one in 20 resolutions correctly. In contrast, DEEPMERGE correctly predicts one in 3 resolutions. On inputs of 3 lines or less, SCANMERGE only achieves 12% accuracy suggesting that the problem space is large even for small merges.
We also compared DEEPMERGE to an out of the box sequence-to-sequence encoder-decoder model (Sutskever, "Sequence to sequence learning with neural networks") (SEQ2SEQ) implemented with FAIRSEQ 3 natural language processing library. Using a naïve input (i.e., concat 3 (A, B, O)), tokenized with a standard byte-pair encoding, and FAIRSEQ's default parameters, we trained on the same dataset as DEEPMERGE. DEEPMERGE outperforms the sequence-to-sequence model in terms of both top-1 (36.5% vs. 2.3%) and top-3 accuracy (43.2% vs. 3.3%). This is perhaps not surprising given the precise notion of accuracy that does not tolerate even a single token mismatch. We therefore also considered a more relaxed measure, the BLEU-4 score (Papineni, "Bleu: a method for automatic evaluation of machine translation"), a metric that compares two sentences for "closeness" using an n-gram model. The sequence-to-sequence model achieves a respectable score of 27%, however DEEPMERGE still outperforms with a BLEU-4 score of 50%. This demonstrates that our novel embedding of the merge inputs and pointer network style output technique aid DEEPMERGE significantly and outperform a state of the art sequence-to-sequence baseline model.
Lastly, we compared DEEPMERGE to JSFSTMERGE (Tavares, "Semistructured Merge in JavaScript Systems"), a recent semistructured AST based approach. JSFSTMERGE leverages syntactic information by representing input programs as ASTs. With this format, algorithms are invoked to safely merge nodes and subtrees. Structured approaches do not model semantics and can only safely merge program elements that do not have side effects. Structured approaches have been proven to work well for statically typed languages such as Java (Apel, "Semistructured merge: Rethinking merge in revision control systems"), (Leßenich, "Balancing precision and performance in structured merge"). However, the benefits of semistructured merge hardly translate to dynamic languages such as JavaScript. JavaScript provides less static information than Java and allows statements (with potential side effects) at the same syntactic level as commutative elements such as function declarations.
As a baseline to compare to DEEPMERGE, we ran JSFST-MERGE with a timeout of 5 minutes. Since JSFSTMERGE is a semistructured approach we apply a looser evaluation metric. A resolution is considered correct if it is an exact syntactic match with R or if it is semantically equivalent. We determine semantic equivalence manually. JSFSTMERGE produces a correct resolution on 3.7% of samples which is significantly lower than DEEPMERGE. Furthermore, JSFSTMERGE does not have support for predicting Top-k resolutions and only outputs a single resolution. The remaining 96.3% of cases failed as follows. In 92.1% of samples, JSFSTMERGE was not able to produce a resolution and reported a conflict. In 3.3% of samples, JSFSTMERGE took greater than 5 minutes to execute and was terminated. In the remaining 0.8% JSFSTMERGE 3. https://github.com/pytorch/fairseq Threshold (Alon, "code2seq: Generating sequences from structured representations of code")(Apel, "Semistructured merge: Rethinking merge in revision control systems") lines (Apel, "Semistructured merge in revision control systems")(Chakraborty, "Tree2tree neural translation model for learning source code changes") lines (Cho, "Learning phrase representations using RNN encoder-decoder for statistical machine translation")(Cho, "Learning phrase representations using rnn encoder-decoder for statistical machine translation") lines (Clement, "Multi-mode translation of natural language and python code with transformers")(Ferreira, "Software engineering meets deep learning: A literature review") lines [>10] lines 0 78.40% 56.50% 37.04% 10.87% 2.93% produced a resolution that was both syntactically and semantically different than the user's resolution. In addition to effectiveness, DEEPMERGE is superior to JSFSTMERGE in terms of execution time. Performing inference with deep neural approaches is much quicker than (semi) structured approaches. In our experiments, JSFSTMERGE had an average execution time of 18 seconds per sample. In contrast, sequence-to-sequence models such as DEEPMERGE perform inference in under a second. Sensitivity to Input Merge Conflict Size. We observe that there is a diverse range in the size of merge conflicts (lines in A plus lines in B). However, as shown in Figure 7, most (58% of our test set) merges are small, consisting of 7 or less lines. As a product of the dataset distribution and problem space size, DEEPMERGE performs better for smaller merges. We present aggregate Top-1 accuracy for the input ranges in Table 2. DEEPMERGE achieves over 78% synthesis accuracy on merge inputs consisting of 3 lines or less. On merge inputs consisting of 7 lines or less (58% of our test set) DEEPMERGE achieves over 61% synthesis accuracy.

## RQ2: Effectiveness of Suppressing Incorrect Resolutions

The probabilistic nature of DEEPMERGE allows for accommodating a spectrum of users with different tolerance for incorrect suggestions. "Confidence" metrics can be associated with each output sequence to suppress unlikely suggestions. In this section, we study the effectiveness of DEEPMERGE's confidence intervals.
In the scenario where DEEPMERGE cannot confidently synthesize a resolution, it declares a conflict and remains silent without reporting a resolution. This enables DEEP-MERGE to provide a higher percentage of correct resolutions (higher precision) at the cost of not providing a resolution for every merge (lower recall). This is critical for practical
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 1.0 Precision, Recall, and F1 score (with 95% confidence intervals) Confidence Threshold Recall, Precision, F1 Precision F1 Recall Figure 8: Top-1 precision and recall by confidence threshold. Class Top-1 Accuracy Percent of Dataset CONCAT 44.40% 26.88% OTHER 29.03% 73.12%
Table 3: Accuracy and Distribution of classes.
use, as prior work has shown that tools with a high false positive rate are unlikely to be used by developers (Johnson, "Why don't software developers use static analysis tools to find bugs?"). Figure 8 depicts the precision, recall, and F1 score values, for various confidence thresholds (with 95% confidence intervals). We aim to find a threshold that achieves high precision without sacrificing too much recall. In Figure 8, the highest F1-Score of 0.46 is achieved at 0.4 and 0.5. At threshold of 0.5, DEEPMERGE's top-1 precision is 0.72 with a recall of 0.34. Thus, while DEEPMERGE only produces a resolution one third of the time, that resolution is correct three out of four times. Compared to DEEPMERGE with no thresholding, at a threshold of .5 DEEPMERGE achieves a 2x improvement in precision while only sacrificing a 10% drop in recall. Thresholds of 0.4 and 0.5 were identified as best performing on a held out validation set. We then confirmed that these thresholds were optimal on the held out test set reported in Figure 8.

## RQ3: Categorical Analysis of Effectiveness

We now provide an analysis of DEEPMERGE's performance.
To understand which samples DEEPMERGE is most effective at resolving, we classify the dataset into two classes: CONCAT and OTHER. The classes are defined as follows:
1) CONCAT -resolutions of the form AB or BA. Specifically:
• R contains all lines in A and all lines in B.
• There is no interleaving between A's lines and B's lines.
• The order of lines within A and B is preserved. 2) OTHER -resolutions not classified as CONCAT.
OTHER samples can be any interleaving of any subset of lines.
Table 3 shows the performance of DEEPMERGE on each class. DEEPMERGE performs comparably well on each category suggesting that DEEPMERGE is effective at resolving conflicts beyond concatenation.
Top-1 Top-3 Naïve 9.62% 14.09% Linearized 15.25% 19.95% LTRE 23.37% 29.21% Aligned Naïve 27.41% 32.22% Aligned Linearized 36.50% 43.23%
Table 4: Accuracy of different input representation choices.

## RQ4: Impact of Input Representation

We now evaluate the use of Merge2Matrix and show the benefit of the Aligned Linearized implementation used in DEEPMERGE.
We evaluate DEEPMERGE on each combination of summarization and edit aware alignment described in Section 3.2: Naïve, Linearized, LTRE, Aligned naïve, and Aligned Linearized. Table 4 shows the performance of each input representation on detection and synthesis. The edit-aware input formats: LTRE, Aligned Naïve, and Aligned Linearized attain an improvement over the edit-unaware formats. Our Aligned representations are more succinct and contribute to a large increase in accuracy over the edit-unaware formats. Aligned Naïve increases accuracy over our best edit-unaware format by 12.16% for top-1 and 12.27% for top-3. We believe this is due to the verbosity of including the underlying tokens as well as the ∆ edit sequence. The combination of our editaware and summarization insights (Aligned Linearized) yields the highest accuracy.

## Summary of Results

Our evaluation and baselines indicate that the problem of synthesizing resolutions is a non-trivial task, even when restricted to resolutions that rearrange lines from the conflict. DEEPMERGE not only can synthesize resolutions for more than a third of times, but can also use its internal confidence to achieve high precision (72%). DEEPMERGE can synthesize resolutions significantly more accurately than heuristic based, neural, and structured approaches. We also illustrate the need for edit-aware aligned encoding of merge inputs to help deep learning be more effective synthesizing non-trivial resolutions.

## RELATED WORK

Our technique is related to several existing works in both program merging and deep learning for code.

## Source Code Merging

The most widely used method for merging changes is diff3, the default for most version control systems. One reason for its popularity is that diff3 is purely text based and therefore language agnostic. However, its behavior has been formalized and Khanna et al. showed that the trust developers have in it may be misplaced (Khanna, "A formal investigation of diff3"), including the examples in Figure 1.
There have been many attempts to improve merge algorithms by taking language specific analyses into account (see the work of Mens for a broad survey (Mens, "A state-of-the-art survey on software merging")). Westfechtel et al. use the structure of the source code to reduce merge conflicts (Westfechtel, "Structure-oriented merging of revisions of software documents"). Apel et al. noted that structured and unstructured merge each has strengths and weaknesses. They developed JSFSTMERGE, a semi-structured merge, that alternates between approaches (Apel, "Semistructured merge in revision control systems"). They later introduced JDIME, an approach that automatically tunes a mixture of structured and unstructured merge based conflict locations (Apel, "Structured merge with auto-tuning: balancing precision and performance"). Sousa et al. introduced a verification approach, SAFEMERGE that examines the base program, both changed programs, and the merge resolution to verify that the resolution preserves semantic conflict freedom (Sousa, "Verified three-way program merge").
The key difference between DEEPMERGE and these structured or semi-structured merge approaches is that they require a priori knowledge of the language of the merged code in the form of a parser or annotated grammar (or more advanced program verification tools). Further, structured merge tools cannot conservatively merge changes made within method bodies. Finally, Pan et al. (Pan, "Can program synthesis be used to learn merge conflict resolutions? an empirical analysis") explore the use of program synthesis for learning repeated resolutions in a large project. The approach requires the design of a domainspecific languages inspired by a small class of resolutions (around imports and macros in C++). In contrast to both these approaches, DEEPMERGE only requires a corpus of merge resolutions in the target language, and can apply to all merge conflicts. However, we believe that both these approaches are complementary and can be incorporated into DEEPMERGE.

## Deep Learning on Source Code

We leverage deep neural network based natural language processing methods to address the challenge of three way merge resolution. We discuss related works in sequence-tosequence learning that inspired our model and applications of deep learning for the software engineering domain.
Pointer networks (Vinyals, "Pointer networks") use attention to constrain general sequence-to-sequence models (Sutskever, "Sequence to sequence learning with neural networks"), (Cho, "Learning phrase representations using RNN encoder-decoder for statistical machine translation"). Recent works incorporate a copy mechanism in sequence-to-sequence models by combining copying and token generation (Gu, "Incorporating copying mechanism in sequence-to-sequence learning"), adding a copying module in the decoder (Zhou, "Sequential copying networks"), and incorporating it into the beam search (Panthaplackel, "Copy that! editing sequences by copying spans"). In contrast to DEEPMERGE, none of these approaches address the challenges described in Section 2 in a three-way merge.
Deep learning has been successfully used on source code to improve myriad software engineering tasks. These include code completion and code generation (Svyatkovskiy, "Pythia: Aiassisted code completion system"), (Clement, "Multi-mode translation of natural language and python code with transformers"), code search (Gu, "Deep code search"), software testing (Godefroid, "Learn&fuzz: Machine learning for input fuzzing"), defect prediction (Wang, "Automatically learning semantic features for defect prediction"), and code summarization (Alon, "code2seq: Generating sequences from structured representations of code"). Deep learning has been used in program repair using neural machine translation (Tufano, "An empirical study on learning bug-fixing patches in the wild via neural machine translation"), (Chakraborty, "Tree2tree neural translation model for learning source code changes"), sequence-editing approaches (Panthaplackel, "Copy that! editing sequences by copying spans"), and learning graph transformations (Dinella, "Hoppity: Learning graph transformations to detect and fix bugs in programs"). For a deeper review of deep learning methods applied to software engineering tasks, see the literature reviews (Li, "Deep learning in software engineering"), (Ferreira, "Software engineering meets deep learning: A literature review").
While neural sequence-to-sequence models are utilized in most of those applications, they consume only one input sequence, mapping it to a single output sequence. Edit aware embeddings (Yin, "Learning to represent edits") introduced LTRE method to encode two program variants to model source code edits. As we demonstrate, our edit-aware encoding Aligned Linearized is inspired by this approach but significantly outperforms LTRE in the context of data-driven merge.
