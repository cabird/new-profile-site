# The Missing Links: Bugs and Bug-fix Commits

**Abstract**

Empirical studies of software defects rely on links between bug databases and program code repositories. This linkage is typically based on bug-fixes identified in developer-entered commit logs. Unfortunately, developers do not always report which commits perform bug-fixes. Prior work suggests that such links can be a biased sample of the entire population of fixed bugs. The validity of statistical hypotheses-testing based on linked data could well be affected by bias. Given the wide use of linked defect data, it is vital to gauge the nature and extent of the bias, and try to develop testable theories and models of the bias. To do this, we must establish ground truth: manually analyze a complete version history corpus, and nail down those commits that fix defects, and those that do not. This is a difficult task, requiring an expert to compare versions, analyze changes, find related bugs in the bug database, reverse-engineer missing links, and finally record their work for use later. This effort must be repeated for hundreds of commits to obtain a useful sample of reported and unreported bug-fix commits. We make several contributions. First, we present Linkster, a tool to facilitate link reverse-engineering. Second, we evaluate this tool, engaging a core developer of the Apache HTTP web server project to exhaustively annotate 493 commits that occurred during a six week period. Finally, we analyze this comprehensive data set, showing that there are serious and consequential problems in the data.

## INTRODUCTION

Software process data, especially bug reports and commit logs, are widely used in software engineering research. The integration of these two provides valuable information on the history and evolution of a software project. It is used, e.g., to predict the number and locale of bugs in future software releases (e.g., (Ostrand, "Predicting the location and number of faults in large software systems")(Śliwerski, "When do changes induce fixes?")(Joshi, "Local and global recency weighting approach to bug prediction")(Bernstein, "Improving defect prediction using temporal features and non linear models")). The two data sources are normally integrated by scanning through the version control log messages for potential bug report numbers; conscientious developers enter this information when they check-in bug fixes (e.g., see (Fischer, "Populating a release history database from version control and bug tracking systems")). We used similar techniques in our previous work, and, in fact, improved current practice by adding heuristics to check the results (Bachmann, "Data retrieval, processing and linking for software process data analysis")(Bachmann, "Software process data quality and characteristics -a historical view on open and closed source projects"). Even so, the links (between program code commits and bug reports) thus extracted cannot be guaranteed to be correct, as they are reliant on voluntary developer annotations in commit logs.
In prior work, we have shown that such data sets are plagued by quality issues (Bachmann, "Software process data quality and characteristics -a historical view on open and closed source projects"); furthermore, these issues (e.g., incompleteness, bias, etc.) adversely affect applications and algorithms which rely on such data (Bird, "Fair and balanced? bias in bug-fix datasets"). We defined two types of bias: bug-feature bias, where only the fixes of certain types of defects are linked, and commit-feature bias, where only the certain kinds of fixes, or fixes to certain kinds of files, are linked. In addition to these data quality issues, many researchers make questionable process assumptions: for instance they assume that all the relevant bugs of a software product are actually reported in the bug tracking database of the project. To truly understand defectreporting bias and verify such assumptions, we must uncover the ground truth: we must analyze completely (at least a time-window of) the commit version history of a project, and precisely identify all the commits that are defect fixes, and those that are not.
To get at ground truth requires skill, knowledge and effort: one must compare successive versions, understand the changes, identify any relevant reported bugs in the repo, and establish a link when possible. This process must be repeated until we have a large enough sample for statistical analysis. This is costly, difficult, and time-consuming.
Linkster is a convenient, interactive tool, integrating multiple queryable, browseable, time-series views of version control history and bug report history. Linkster enables an expert to quickly find and examine relevant changes, and annotate them as desired; specifically, Linkster makes it easy to find defect-fix commits. We engaged an expert Apache core developer, Dr. Justin Erenkrantz, to use Linkster to manually annotate 6 full weeks (including 493 commit messages) of the Apache history. This case study helped us to improve the tool, and yielded a trove of data to examine three research questions.
Traditionally, researchers have made several assumptions about the bug fixing, reporting, and linking phenomena. The first two research questions reflect general internal validity concerns that arise when using linked bug data for software engineering research.
RQ 1: Do the bug reporting and fixing practices of developers correspond to the assumptions commonly made by researchers?
Second, researchers have tended to gloss over the issue of whether automated tools that find links between commits and bug reports have false-positives or false-negatives.
RQ 2: How well does the automated approach of finding links between commits and bug reports work? Finally, the linked set of bug-fixing commits are a sample of the full set of bug-fix commits. We can check and see if this sample is biased in any detectible way. RQ 3: Is there any evidence of systematic bias in the linking of bug-fix commits to bug reports (Bird, "Fair and balanced? bias in bug-fix datasets")?
To our knowledge, the only published study on this question is by Aranda and Venolia (Aranda, "The secret life of bugs: Going past the errors and omissions in software repositories"): they analyzed the completeness and degree of truth in software engineering datasets and provided a partial answer to RQ 1 (see Sub-Section 2.2). Most studies do not even address data quality issues (Liebchen, "Data sets and data quality in software engineering").
In addition, we were able to qualitatively explore how the Apache project actually uses software engineering tools such as bug tracker and version control systems, yielding some rather surprising observations.
We begin with a discussion of related work (Section 2), followed by an overview of the tools and processes (Section 3) used in Apache HTTP web server project. We then present (Section 4) a description of Linkster, and details of the case study procedure evolving an Apache core developer (Section 5). In Sections 6 and 7 we present our findings, which we summarize briefly below: Finding 1: A so-called "bug" is not always a bug; neither is a "commit" always a commit. In other words: in Apache, the most important bugs are not handled in the bug tracker but mentioned in the mailing list system; and only a fraction of commits actually pertain to program changes (RQ 1).
Finding 2: We compared the manual annotations with data produced by automated linking (viz., for false-positives or false-negatives); the automated approach finds virtually all the commit log messages which contain a link to the bug tracking database (RQ 2). Sadly, however, many defect-fix commits are un-identified in the commit logs, and thus are invisible to automated approaches.
Finding 3: In the manually annotated sample, we find strong statistical evidence that different bug-fixers vary in their linking behavior. Investigating further, we find anecdotal evidence suggesting that factors such as experience, ownership and the size (number of files) of the commit affect linking behaviour. We also find that reporting bias affects the performance of a bug prediction algorithm (BugCache). Given the small size of the manually annotated sample, the evidence here is mostly suggestive rather than statistically significant; however, it points out the strong need for further studies-for if this type of reporting bias is confirmed as a widespread problem, this is of serious, fundamental concern to all empirical research that uses this type of linked bug-fix data.

## RELATED WORK

Areas closely related to this research include data extraction and integration, data quality in software engineering, data verification in software repositories, and our own previous work on data quality effects on empirical software engineering.

## Data Extraction and Integration

Software engineering process data such as bug reports and version control log files are widely used in empirical software engineering. Therefore, the extraction and integration of this data is critical.
Fischer et al. (Fischer, "Populating a release history database from version control and bug tracking systems") presented a Release History Database (RHDB) which contains the version control log and the bug report information. To link the change log and the bug tracking database, Fischer et al. searched for change log messages which match to a given regular expression. Later, they improved the linking algorithm and built in a filemodule verification (Fischer, "Analyzing and relating bug report data for feature tracking"). A similar approach to link the change log with the bug tracking database was chosen by other researchers. All of them used regular expressions to find bug report link candidates in the change log file (e.g., (Čubranić, "Hipikat: Recommending pertinent software development artifacts")(Śliwerski, "When do changes induce fixes?")(Schröter, "If your bug database could talk")(Zimmermann, "Predicting defects for eclipse")(Zimmermann, "Preprocessing cvs data for fine-grained analysis")(Schröter, "If your bug database could talk")).
In (Bachmann, "Data retrieval, processing and linking for software process data analysis"), we presented a step-by-step approach to retrieve, parse, convert and link the data sources. We improved the well-established prior art, enhancing both the quality and quantity of links extracted.

## Data Quality in Software Engineering

As discussed in (Bird, "Fair and balanced? bias in bug-fix datasets"), empirical software engineering researchers have considered data quality issues. Space limitations inhibit a full survey, we present a few representative papers.
Koru and Tian (Koru, "Defect handling in medium and large open source projects") surveyed members of 52 different medium to large size Open Source projects with regards to defect handling practices. They found that defect-handling processes varied among projects. Some projects are disciplined and require recording of all bugs found; others are more lax. Some projects explicitly mark whether a bug is pre-release or post-release. Some record defects only in source code; others also record defects in documents. This variation in bug datasets requires a cautious approach to their use in empirical work. Liebchen et al. (Liebchen, "Filtering, robust filtering, polishing: Techniques for addressing quality in software data") examined noise, a distinct, equally important issue.
Liebchen and Shepperd (Liebchen, "Data sets and data quality in software engineering") surveyed hundreds of empirical software engineering papers to assess how studies manage data quality issues. They found only 23 that explicitly referenced data quality. Four of the 23 suggested that data quality might impact analysis, but made no suggestion of how to deal with it. They conclude that there is very little work to assess the quality of data sets and point to the extreme challenge of knowing the "true" values and populations. They suggest that simulation-based approaches might help.
Bettenburg et al. (Bettenburg, "Quality of bug reports in eclipse")(Bettenburg, "What makes a good bug report?")[?] provided first analysis of bug report quality. They investigated the attributes of a good bug report surveying developers and used it to develop a computational model of a bug report quality. The resulting model allowed to display the current quality of a defect report whilst typing. Hooimeijer et al. (Hooimeijer, "Modeling bug report quality") also analyzed the quality of defect reports and tried to predict whether the defect report will be closed within a given amount of time.
Chen et al. (Chen, "Open-source change logs") studied the change logs of three Open Source projects and analyzed the quality of these log files.
In (Bachmann, "Software process data quality and characteristics -a historical view on open and closed source projects") we surveyed five Open Source and one Closed Source project in order to provide a deeper insight into the quality and characteristics of these often-used process data. Specifically, we defined quality and characteristics measures, computed them and discussed the issues arose from these observation. We showed that there are vast differences between the projects, particularly with respect to the quality of the link rate between bugs and commits.
Aranda and Venolia (Aranda, "The secret life of bugs: Going past the errors and omissions in software repositories") provided a field study of coordination activities around bug fixing, based on a survey of software professionals at Microsoft. Specifically, they studied 10 bugs in detail and showed that (i) electronic repositories often hold incomplete or incorrect data, and (ii) the histories of even simple bugs are strongly dependent on social, organizational, and technical knowledge that cannot be solely extracted through the automated analysis of software repositories. They report that software repositories show an incomplete picture of the social processes in a project. While they studied 10 bugs in detail, we focus on commit history: we employed an expert, supported by a specially-designed tool to fully annotate a sample of 493 commits. This data helped us uncover a) some of the weaknesses of software repositories as well as b) anecdotal evidence of systematic bias in bug-fix reporting.

## Studying Bias

Papers in empirical software engineering rarely tackle data quality issues directly (see discussion earlier in this section); our earlier work is an exception. In (Aune, "Looking back on prediction: A retrospective evaluation of bug-prediction techniques") and (Bird, "Fair and balanced? bias in bug-fix datasets") we investigated historical data from several software projects, and found strong evidence of systematic bias. We then investigated potential effects of "unfair, imbalanced" datasets on the performance of prediction techniques.
Ideally, all bug-fixing commits are linked to bug reports; then empirical research would consider all type of fixed bug reports. However only some of the fixed bugs have links to the bug-fixing commits. This raises the possibility of two types of bias: bug feature bias, where only certain types of bugs are linked, or commit feature bias, whereby only certain types bug-fixing repairs are linked. Either type of bias is highly undesirable. With access to all the fixed bugs, and the linked bugs, we could check for bug feature bias. Our study (Bird, "Fair and balanced? bias in bug-fix datasets") suggested that bug feature bias does exist, and also that it affects the performance of the award-winning BugCache defect prediction algorithm (Kim, "Predicting faults from cached history"). In this work, we have a fully annotated list of commits for the first time, thus achieving "ground truth" for a subset of the Apache dataset, and thus we can analyze the data for commit feature bias.
In summary: a few studies explicitly consider the quality of systematic bias in the data. This study, in contrast, explores the implications of this behavior by attempting to unearth the ground truth by enlisting a core developer to annotate all commits, and thus seek out quality and bias issues.

## CASE STUDY: APACHE

The Apache HTTP web server is an Open Source software system developed under the auspices of the Apache Software Foundation. Apache is the most popular web server on the Internet, serving over 55% of all websites (Ltd, "web server survey"). Apache is also one of the most popular Open Source projects among researchers. It is widely used in current empirical software engineering research (e.g., (Mockus, "Two case studies of open source software development: Apache and mozilla")(Paulson, "An empirical study of open-source and closed-source software products")(Ko, "A linguistic analysis of how people describe software problems")(Bettenburg, "What makes a good bug report?")(Just, "Towards the next generation of bug tracking systems")), and thus a good subject for an in-depth examination of data quality.

## Project Tools

Like many other Open Source projects, Apache uses the BugZillafoot_0 bug tracker and the SVNfoot_1 version control system. In addition, the Apache Software Foundation provides officially maintained gitfoot_2 mirrors for all projects. The Apache project allows free access to the contents of all these tools. Apache also maintains a public mailing list for developers and Apache users to discuss issues of concern.

## Data Gathering and Integration

We retrieved, processed and linked the Apache HTTP web server process data as presented in (Bachmann, "Data retrieval, processing and linking for software process data analysis"). Basically, we downloaded all BugZilla bug reports and SVN version control log files. Then, we scanned each commit log message for indications of fixing a bug using a set of heuristics; typically we look for bug report numbers in log messages. This leads to a set of automatically extracted links between program code commits and bug reports. This set of links is validated using another set of heuristics (op cit).

## Apache Dataset

With our own (rather modest) resources, we could only completely evaluate and manually verify a subset of the original Apache dataset. Therefore, we had to sample the original dataset. There were two choices: random sampling or temporal sampling.
Random sampling requires some rationale for selecting a sample-e.g., prior knowledge of the distribution of the relevant co-variates to the study, so that a sample representative of the population could be chosen. It is difficult to decide a priori what such co-variates might be, let alone their distribution. So, we chose to perform temporal sampling. With this approach, we chose to verify all the commits in a given period. With complete results for that period, we can then revisit our earlier results and judge the quality against this limited but complete and accurate temporal sample. To find a "typical" period for our evaluation dataset we analyzed the whole original Apache dataset based on week-long epochs. Then, we chose a period of 6 consecutive weeks that was as representative as possible to the overall original Apache dataset in terms of its descriptive process statistics (e.g., similar proportions of bugs and commits). Table 1 lists some basic software process statistics for both-the original and the evaluation-Apache datasets including the finally defined time-frames.

## LINKSTER

The use of Linkster simplified our domain expert's task, greatly accelerating an otherwise tedious, repetitive and inconvenient sequence of invocations of multiple tools.
Figure 1 shows a screenshot of Linkster, showing windows containing three kinds of information: commit transactions including all the changed files (a), bug reports (b), and diff & blame information for all of the lines in a file before and after a particular commit (c).
Linkster requires access to a version control system for file content and a database (local or remote), containing the raw mined repository and bug tracking information. We use git as our backend repository format, given its increasing popularity (Bird, "The promises and perils of mining git"), and ready availability of tools supporting conversion from competitors such as CVS, SVN, etc. However, for convenience, Linkster displays the revision IDs from the original repository. All notes, links, and annotations (explained below) made by the user are also recorded in the database to facilitate use and analysis thereof after annotation. Linkster efficiently displays, integrates, and allows inspection and annotation of information from all data sources. Linkster is written in Python, using the PyQt widget toolset and has been written with portability in mind. We have successfully run it on Linux, OS X, and Windows.
To our knowledge, no other tool provides integrated project information in combination with functionality to annotate / link commits. Hipikat (Čubranić, "Hipikat: Recommending pertinent software development artifacts"), which was developed at UBC, is similar in that it creates links between different types of software artifacts. However, these links are based purely on heuristics and Hipikat functions as a recommender system rather than a browsing and annotation system.
Other tools such as EvoLens, softChange, or Shrimp provide only part of the functionality, but all existing tools have goals other than expert commit annotation.
SoftChange (German, "Mining cvs repositories, the softchange experience") is a tool to aid software engineering research by visualizing data. Similar to Linkster, SoftChange integrates data from multiple sources such as version control systems, releases, and bug databases. However, softChange uses visualizations (usually plots) to answer questions,(e.g., how many bugs are closed in each time period?) and does not allow annotation of data as Linkster does.
EvoLens (Ratzinger, "Evolens: Lens-view visualizations of evolution data") helps developers to understand the evolution of a piece of software by visualizing the software as well as metrics of the software over time. The visual nature across time facilitates identifying design erosion and hot spots of activity. Linkster does not leverage advanced visualization techniques and integrates multiple types of data rather than just source code information.
Shrimp (Michaud, "Integrating information sources for visualizing java programs") integrates and visualizes source code, docu-mentation (Javadoc), and architectural information to aid source code exploration. Linkster is more concerned with process related artifacts, (e.g., changes, discussions, bug reports, and fixes) than understanding the source code itself.

## Commit Information

Figure 1-a shows the Commit Information Window of Linkster. The top (1) contains a list of commits that satisfy some query, e.g., commits within a time window or changes made by a particular author. Each line shows the revision identifier (as used in the original repository), commit time, author, and the first line of the commit message. The entire commit message is shown in a tooltip when the mouse hovers over an entry.
When a commit entry in the list is selected, the metadata is updated in the bottom half (Aune, "Looking back on prediction: A retrospective evaluation of bug-prediction techniques"). The list of files modified in the commit (3) is also displayed. Double clicking a file brings up the Blame & Diff Information for the file allowing the user to examine the exact changes that were made. For annotation purposes, the user may select the reason(s) for the commit by checking boxes (4) or drag and drop (or remove) a bug record from the Bug Information Window into the list of bug IDs (5), which is populated with the set of automatically identified links between the commit and bug records. Finally, the user may enter free form notes for the commit (6). 7) is a scrollable list of bugs from the bug database. Each entry contains the bug ID, the date of creation, and a one line summary of the bug. Hovering over an entry shows the bug severity in a tooltip. Any of these entries may be dragged to the bug IDs list (5) in the commit information window to indicate a commit that is associated with the bug.

## Bug Information

Selecting a bug entry populates the bottom half of the window with detailed information. The left side (8) contains short attributes of the bug, while the right side [?] displays the full bug description followed by all of the comments in chronological order with author and date. Clicking on the Bug Activity tab (10) displays a list (not shown) of all changes to the bug record, such as assigning the bug to a developer or marking a bug as closed. Each entry indicates when the change was made and who made it along with old and new values for the changed field as appropriate. Finally, clicking on the Fixing Files tab (11) presents a list (not shown) of all of the commits to files that are associated with the fix of the bug. This list is comprised of files automatically or manually linked to the bug. Double clicking on any file in this list will bring up a blame & diff window for the commit. for the changes to a file in a particular commit. The left view (Fischer, "Analyzing and relating bug report data for feature tracking") shows the content of the file prior to the change, and the right view (Fischer, "Populating a release history database from version control and bug tracking systems") shows the content after the change. Removed lines are prefixed with "-" and are highlighted red, and added lines are in green with a "+" prefix. Each line is also prefixed with revision identifier of the commit that introduced the line. Selecting a line highlights all other lines introduced in the same commit, and also updates the metadata area (Chen, "Open-source change logs") with information about that commit. This can help the user learn why, when, and by whom, the line was originally added. If additional information is desired, double clicking a line will bring up a new Blame & Diff window for the commit which introduced the line (if, for example, one desires to see why a line that was removed in one revision was originally added in a prior revision). An annotator can, thus, gradually step back through version history.

## Blame & Diff Information

The views are synchronized such that scrolling up, down, left, or right in one view causes the other to change accordingly. The thumbnail view (German, "Mining cvs repositories, the softchange experience") graphically shows the differences for the entire file with red indicating removed lines and green, added lines. Clicking on a location in the thumbnail view will cause the pre and post views to jump to that location, making it easier to identify and examine changes in larger files.

## APACHE DATA EVALUATION

To address our research questions, we began our evaluation with the creation of an evaluation dataset, as defined in Section 3.3. Armed with Linkster to facilitate browsing and annotation, we engaged the services of an informant: an experienced Apache developer, Dr. Justin Erenkrantz, to manually annotate a temporal sample of commits using Linkster. Clearly, the quality of this completely annotated evaluation dataset is predicated on the expertise of the annotator. Justin is a core developer of the Apache HTTP web server project (since January 2001), the President of the Apache Foundation and serves on the Foundation's Board of Directors. He also develops for Apache Portable Runtime, Apache flood and Subversion 5 .
Using Linkster, Justin annotated each commit, to flag it as a bug fix, an implemented feature request, a maintenance task or other. With this information, we obtain fully annotated commit data, providing a complete picture of all the changes during the given period and how/why/by whom these changes were made. This data can be used to verify our automated linking approach (which includes mainly bug fixes and some feature requests). Indeed, annotating program code commits dating back months or years in the past is a challenge, even for an experienced core developer like Justin. Linkster was very helpful, providing an integrated view of all the relevant information. Based on the log message, the changed files and the file diffs of the changed files, Justin was able to annotate all commits, and, in most cases, provided additional information about the commits.
Justin's familiarity with the Apache project gives us confidence that the results of our evaluation can be trusted. In addition, detailed discussions and interviews with him revealed facts about the tools and processes used in the Apache HTTP web server project, and also ideas for improving Linkster.

## RESULTS

All 493 commits in our selected temporal sample were annotated. In addition to the annotation into the four categories above: bug fix, feature request, maintenance/refactoring, and other, our informant helped us further sub-classify the commits. Table 2 summarizes the annotation results including the sub-classification. Note, a single commit can have many annotations, e.g., a commit may be annotated as both a "bug fix" and a "feature request". Based on Justin's insights into the Apache development process, we developed a second, orthogonal categorization that was more consistent with the procedures within the project (Table 3). In contrast to our categorization, this one assigns each commit exclusively to one of its processspecific categories: backport/forward port, security fix, bug fix, documentation, voting, release, or other. In the following sub-sections, we present our findings relative to the research questions presented in Section 1. We also present additional findings based on interviews with Justin.

## Bugs Incognito

Contrary to conventional wisdom, participants of the Apache project do not report all the bugs solely through BugZilla. We found that developers and professional users also make use of the Apache mailing list to report bugs and provide bug fixes (sometimes at the same time) without reporting them in the bug tracker.
Finding 1. Not all fixed bugs are mentioned in the bug tracking database. Some are discussed (only) on the mailing list.
As shown in Table 2, we have 82 bug fix related commits in our evaluation dataset. 32 of them (bug report) are directly related to the bug tracking database. 7 other commits contain a bug-fix, but are not the initial bug fix commit rather than a merge of versions which contain bug fixes indirectly (bug report (merge)). This means, that only 47.6% of bug fix related commits ( 32+7 82 ) are documented in the bug tracking database. For 13 other commits (16% of total) identified by Justin as bug fixes, there are related discussions in the Apache mailing list. This leads to the discouraging observation that many bugs never appear in the bug tracking database, but rather are only discussed on the mailing list. Such a discussion often includes the bug fix provided by a non Apache core developer. According to Justin, these bugs are often the very important bugs especially because of the high attention by Apache developers and the core community on the mailing list. Note also that reporting some types of bugs (e.g., security related ones) on the mailing list is a practice explicitly requested by the Apache Foundationfoot_5 .
Unfortunately, even knowing about the mailing list bugs, it is hard to i) identify and ii) automatically mine them or extract information similar to a bug report stored in the bug tracking database (such as status changes, priority, severity, etc.). Apache SVN revision #291558 (see Figure 2), for instance, is related to a bug discussed on the mailing list 7 . If one were to inspect the mailing list message, one would find almost no evidence that this was a bug fix.
Finally, Justin found 17 other bug-fixing commits (21%) which have neither an associated bug report or mailing list message. This phenomenon, of under-reporting of bugs, is a big problem. If important bugs are excluded from experimental data (i.e., many bugs are left out) then the effectiveness of defect prediction models and the validity of statistical studies (which rely on them being in the bug tracking database) may be threatened. This leads to the conclusion, that not all fixed bugs are reported as bugs in the bug tracking database, or in other words: bugs go "incognito".

## Backport Incognito

In the Apache HTTP web server project only a few developers are allowed to commit to an Apache release version: thus a bug-fix on one release may actually have to be committed by someone else to an older or different release. Typically, this process works as follows. First, a developer fixes a given bug and commits the new version to the current version under active development (also known as the "trunk"). Ideally s/he also refers to the related bug report in the commit log. Next, at least two other developers review the changed code, verify the changes and vote either for or against the fix (this step is related to the voting commits as shown in Table 2 and 3). Finally, if the votes are positive, the fix is committed (or merged) to Apache release versions, which is called a backport. As a result of this process, we might find several different commits in the version history, that fix the same bug.
Finding 2. To fix a bug in an Apache release, multiple similar commits by different developers are needed.
Unfortunately, backport commits are not that easy to identify by existing linking algorithms and heuristics; frequently, while the log message for original commit to the trunk refers to the bug report, the backport commit log does not. To worsen matters, after the bug is actually closed, there is a rigorous review, verification and voting process before the backport is accepted and committed. Therefore, the time difference between the backport commit and the status change (to fixed) on the bug report may rise to several days, which again, makes it difficult to link the bug with the commit. As a result, automated linking algorithms will largely ignore backport fixes. Arguably, these are fixes are very important: often they are involved in post-release failures. They should not be ignored by researchers engaged in hypothesis testing or defect prediction work. Alas, finding them may require extensive, high-expertise combing through commit histories.

## Impact-of-Defect vs. Cause-of-Defect

This is a thorny issue: a defect in one project's code base might actually manifest as a failure in a different project. Thus, some of the reported bugs in Apache HTTP web server have their root-cause outside of the Apache program code. Apache uses external libraries, as well as Apache Commons modules. Therefore, failures in the Apache HTTP web server, even if duly reported in the Apache bug tracking database, may actually have to be fixed elsewhere. The reverse is also possible.
The mod-pythonfoot_7 sub-project maintains its own version control system repository and an Apache project's main bug tracker independent Jira issue tracker 9 . Mod-python issue 83foot_9 , for instance, was reported in the Jira issue tracker but fixed in the Apache program code.
Finding 3. Developers sometimes fix bugs that are only reported in some other projects' bug tracker, rather than in their own; and vice-versa.
Ideally, we have a complete, integrated source of all the bugs in the bug repository, and all the fixes in the version control system. Our findings, and indeed, the widespread prevalence of cross-project module reuse, we can expect that this type of separation between causes and effects of defects is quite common. Given this, it would be helpful if a report of a bug impacting one system would be transferred to the bug repository of the causing system, and linked to fix in the version control of that system. However, given the poor linking behaviour when the cause and effect are in the same system, we might expect that this type of cross-system linking is pretty unlikely to occur.

## Commits Incognito

In earlier work (Bachmann, "Software process data quality and characteristics -a historical view on open and closed source projects"), we encountered the problem of unexplained commits, e.g., due to empty commit log messages. Sadly, even an experienced developer would find it difficult to retrospectively reconstruct the explanation of an unexplained commit.
Finding 4. Even if we annotate all commits, the cause of a commit still remains unspecified in some cases.
Table 2 and 3 show the annotation, sub-classification and process-oriented classification of all the commits in our evaluation dataset. Based on the values in Table 3, for 110 commits (22.3%) we have a process specific annotation of other. The reason for these commits, therefore, is not justified by one of the Apache software engineering core tasks.
In addition, most of the commits are not justified by a bug fix or feature request rather than for documentation (32%), voting (5.3%) or releases (8.9%). Only 37.1% of all commits have a functional impact on the software product (feature requests and bug fixes including all backport), which leads us to the conclusion that not all commits are commits that actually change the software.
For additional information to the quality and characteristics of the version control data, we refer to our previous work presented in (Bachmann, "Software process data quality and characteristics -a historical view on open and closed source projects").

## Performance of the Linking Algorithm

In earlier work (Bachmann, "Data retrieval, processing and linking for software process data analysis")(Bachmann, "Software process data quality and characteristics -a historical view on open and closed source projects")(Bird, "Fair and balanced? bias in bug-fix datasets")(Bachmann, "When process data quality affects the number of bugs: Correlations in software engineering datasets"), we reported a linking algorithm whose performance was found to be best-in-class. The fully annotated data provided the first known oracle to evaluate linking algorithms, and so we evaluated ours.
Finding 5. The algorithm (op cit) finds most of the commit log messages that the developers linked to bugs reported in the bug tracker, subject to the time constraints used by our algorithm.
In the chosen temporal sample, our linking algorithm found 29 links between the commit messages and the bug tracking database. Justin also identified all these links; we thus found no false-positive links in our evaluation dataset. In addition to these, Justin found 10 additional links. Seven did not satisfy our heuristic for valid links (time constraint of ±7 days between commit and status change on the bug report), and so our algorithm rejected them as invalid links. Hence, we found three false-negative links in our evaluation dataset. The seven invalid links resulted from backport commits (as explained earlier, Sub-Section 6.2). These backports corresponded to bug-fix links in the original trunk which in fact, were successfully discovered by our algorithm.
Unfortunately, as we elaborated before, even with a high linking rate between the commit messages and the bug tracker, only a subset of the fixed bugs are considered. Hence, bugs discussed on the mail discussion system are often left out by automated linking approaches.

## Performance of LINKSTER

Linkster performed mostly as expected and Justin was able to annotate all the commits (493) of our evaluation sample dataset in one working day. In the discussions with Justin, we found some minor issues, which were promptly remedied. In addition, we found that the most important bugs are discussed in the mailing list system only. Therefore, Linkster has been extended to support browsing of messages from development mailing lists and also enables linking them to both bug reports and repository commits.

## Threats to Validity

This sub-section discusses external and internal threats to validity that can affect the results reported in this section.
Threats to external validity. Can we generalize from the results based on the Apache HTTP web server dataset to other datasets? Software engineering tools and processes vary in different projects and, therefore, our findings based on Apache may not generalize. However, our findings indicate that developers may use software process support tools for various goals not envisioned by its original developers (such as version control systems for voting or mailing list systems for bug reporting). It seems prudent to assume that the Apache project is not a complete exception and that, therefore, the data used in studies of other projects may also lack important information. Another threat is the use of a single annotator (Justin). Getting the same data annotated by other developers, and checking agreement, would have been better; we hope to do this in future work.
Threats to internal validity. Did we choose our evaluation dataset well, and properly analyze it? We chose our time-frame carefully; however, it may not properly represent the original Apache dataset. The annotation and classification were performed carefully by a very experienced Apache core developer. Still, there may be errors. Nonetheless, according to Justin, the interesting practices of the Apache developers are by no means exceptional to this time period.

## COMMIT-FEATURE BIAS, REVISITED

The manual annotation effort indicates that many bug fixes are not identified in the commit logs, and thus are completely invisible to the automated linking tools used to extract bug-fix data. Thus the linked bug-fix commits are a sample of the entire group of commits. However, samples thus extracted have been central to many research efforts. The natural question is: is this sample representative, or biased? We seek to test for the two kinds of bias: bug feature bias, whereby only fixes to certain kinds of bugs are linked, and commit feature bias whereby only certain types of commits are linked (Bird, "Fair and balanced? bias in bug-fix datasets"). Earlier, with access to the entire set of fixed bugs, and the subset of linked bugs, we could check for (and did find) bug feature bias; lacking access to a fully annotated set of commits that tells us which commits are bug fixes, we were previously unable to check for commit feature bias. Now, with a fully annotated temporal sample of commits, we can indeed check for commit feature bias. Commit features are properties of the file and its revision history, such as size, complexity, authorship, etc.. These are critical properties that have been studied in dozens of papers that test theories of bug introductions; they are also the features used for bug prediction. So it is important to test for commit feature bias, and evaluate its impact. In this section, we describe some findings related to commit feature bias, and its effect on a well-known bug-prediction algorithm (BugCache).
We remind the reader that our sample size (despite the time and effort required to gather even that much) is not big enough to realistically expect to find statistically significant support for answers to the questions discussed in this section. However, there are some takeaways: we do find statistical support for the answer to one question, and we do find some anecdotal answers for the other questions. Furthermore, actual bias along any of the lines discussed here would have a highly deleterious effect on the external validity of theories tested using only the linked data. Most importantly, we hope to convince the reader that such studies are important and need to be repeated and conducted at larger scales.

## Sources and Extent of Commit Feature Bias

The first question arises naturally from the fact that there are different individual developers, who may have different attitudes towards linking. The simplest and most obvious question is as follows: Do different developers show significantly different linking behaviour? The anonymized table of developers' linking behavior indicates that this is the case: (p 0.002).
Name Linked Not Linked Name Linked Not Linked a 0 6 b 10 5 c 1 1 d 11 8 e 0 3 f 0 1 g 0 3 h 0 5 i 2 7 j 0 3 k 0 2 l 0 1 m 0 2 n 0 1 o 0 1 p 1 0 q 4 0 Total 26 52
We now hypothesize several different specific possible motivational theories of linking behavior. In several cases, there was a visually apparent signal, in boxplots, albeit none that were statistically significant. The results are shown in Figure 3. We list them below, but we caution the reader to interpret all these findings as at best anecdotal. However, it is important to bear in mind that actual bias influenced by any of the processes hypothesize below would be very damaging to the external validity of theories tested solely on the linked data. Does the experience of the author(s) whose code is being fixed influence linking behaviour? We hypothesized that the quest for greater reputation might incentivize people to link fixes when the code under repair belonged to an experienced (and thus more reputable) person. We measured the fixed code's "author reputation" as the geometric of the prior commit experience of everyone who contributed to the fixed code. The left most boxplot in Figure 3 is weakly suggestive that fixes made to code with more experienced authorship are more likely to be linked. Does the number of files involved in the bug fix matter? If more files are repaired in a bug fix, perhaps the fix is more "impactful"; this might motivate the fixer to more carefully document the change. In fact, the boxplot (second from left in Figure 3) is suggestive that this might be the case, with all the unlinked fixes being single-file fixes. Are more experienced bug fixers more likely to link? We might expect that more experienced developers behave more responsibly. We measure experience as the number of prior commits. The boxplot (second from right) suggests support for this theory, with a noticeably higher median for the linked case. Are developers who "own" a file more likely to link bug-fixes in that file? One might expect that people fixing bugs in their own files are more likely to behave responsibly and link; on the other hand, there is a anti-social reputation-preserving instinct that suggests that they may be less likely to link. We measure ownership as the proportion of lines in the file authored by the bug fixer. Indeed, the boxplot visually supports the "anti-social" theory.
We created plots to evaluate two other theories: Are bug q q q q q q q q q Linked Not Linked 0 500 1000 1500

## Apache -Weighted Experience of Revision

Weighted Experience (Commit Count) q q q q q Linked Not Linked fixes to bigger files more likely to be linked? and Does the prior experience of the file owner influence linking behaviour? and found no informal visual evidence supportive of these theories.

## Practical Effects: BugCache Revisited

The above analysis shows that the extent of bias in the data is significant and that the effort of finding the ground truth (e.g., through manual annotation with Linkster) leads to important insights. But do those insights translate to practical impact? In this sub-section we investigate the impact of approaching ground truth in terms of changes in the accuracy of the award-winning BugCache algorithm (Kim, "Predicting faults from cached history"). To that end, we repeated our experiment showing the impact of bias using Apache data (Bird, "Fair and balanced? bias in bug-fix datasets"). Specifically, we departed from two different datasets: The first dataset (called A below) contained all 1576 bugs introduced in the Apache 2.0 branch. The second one contained the additional 65 bugs found by Justin (called J). Table 4 shows the resulting accuracies for training and predicting on each combination of these two datasets.
Consider training on the extracted data A and predicting on the same data. This provides a baseline accuracy of 0.875. If the prediction is, however, performed on the dataset representing ground truth for the period of manual annotation A ∪ J then the accuracy falls to 0.870. We accede that due to the limited manually annotated period the difference-like all the differences in the table-is not significant. But as the following shows we can recognize a tendency. Alternatively, consider adding the manually annotated bugs to the training set (i.e., training on A ∪ J). In each possible prediction target (i.e., A, J, and A ∪ J) we find that the availability of the additional information actually leads to an improvement in prediction accuracy. This is especially impressive where the prediction target is A as it shows that the manually annotated bugs actually contain information relevant to the automatically extracted ones helping BugCache to find four additional bugs.

## DISCUSSION AND CONCLUSIONS

In this paper, we analyzed three main research questions and tried to find "ground truth" in the commit annotations of a very popular software engineering dataset. We used temporal sampling to define an evaluation subset of the original Apache dataset and manually annotated all commits, with the assistance of an Apache core developer and the use of Linkster. As presented in our previous work, bias in empirical software engineering datasets may affect results of applications which rely on such data (Bird, "Fair and balanced? bias in bug-fix datasets"). Unfortunately, based on our data verification, we found that things are even worse: our findings cast doubt on some of the core assumptions made in empirical research. Specifically:
1. Bugs often go incognito as they are not always reported as a bug in the bug tracker but, e.g., in mailing lists, and 2. commits not always clearly change the functionality of the program.
Specifically, we showed that not all fixed bugs are reported in the bug tracking database and most of the commits (62.9%) are not related to a bug fix or feature request (which would introduce a program change) rather than for documentation (32%), voting (5.3%), or releases (8.9%). In addition, we presented the curious case of backport commits and the challenging impact-of-defect vs. cause-of-defect problem.
Both issues have an impact on software engineering datasets. Consequently, even though automated linkage tools are able to connect a remarkable number of commits to bugs reports, many bugs-sometimes the most critical ones-never show up in the bug tracker and are, therefore, not linked. This raises new issues concerning the validity of studies that rely on version control and bug report data only-beyond what we reported earlier (Bird, "Fair and balanced? bias in bug-fix datasets"). We presented a detailed examination of the bias in automatically linked set, when compared to the manually linked set. Especially notable is the significant variation in linking behavior among developers, and the anecdotal evidence suggesting that bug-fixing experience and code ownership play a role in linking behaviour. We also showed that BugCache has a strong tendency to miss predictions if it is not trained on ground truth. Another implication of the work presented here is that empirical software engineering studies will need to take the whole software development social eco-system (revision control system, bug tracking database, mailing list systems, email discussions, discussion boards, chats, etc. as well as these data from other, related projects) into account in order to elicit a more complete picture of the underlying development process. This would allow to capture the nature of some of the bugs and commits that our informant tediously collected manually.
Nonetheless, this study is only a first step towards qualityapproved datasets and we acknowledge that we were only able to verify a small subset of the overall Apache dataset. Therefore, we hope to influence the community to seek more ground truth for more software engineering datasets. Granted, such work would entail a significant manual labor, but, undoubtedly, the resulting valuable improvements in data fidelity will serve the community well in years to come. We seek mechanisms for fostering this community effort, and welcome suggestions from readers to this end.
