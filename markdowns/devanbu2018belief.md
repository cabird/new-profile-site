## Belief and Evidence

How Software Engineers Form Their Opinions Prem Devanbu, Thomas Zimmermann, and Christian Bird AFTER PROGRAMMING FOR a combined 90 years, and having studied industrial and/or open source software development for more than 60 of those years, we've learned (and then sometimes unlearned) a great many things about our profession and colleagues. However, one observation has firmly endured: developers are stubborn, with passionately held beliefs. Most developers hold certain truths (about programming or software engineering) to be selfevident. However, "self-evident" here (unlike in the US Declaration of Independence) doesn't refer to eternal truths: it refers to individual developers and their self-directed notion of what they "know" to be true.
You've surely heard many of these self-evident "truths." For example, dynamic typing is inherently evil. Dynamic typing is the eternal salvation of humanity. Coding standards are soul-destroying and wasteful. Coding standards have prevented the downfall of Western (or Eastern) civilization. Assertions are for unit tests; injecting them elsewhere is mere superfluous sanctimony. Coding without prolific assertions is evidence of willful foolhardiness. You should hire the most experienced developers you can find. Experienced developers are rooted in old ways and lack creative energy. You probably hold some of these beliefs yourself … and you know you're right and that those who think otherwise are just, well, misinformed.
Certainly, developers aren't unique in this; it's well known that changing people's minds is difficult. (Tan, "Winning Arguments: Interaction Dynamics and Persuasion Strategies in Good-Faith Online Discussions") However, shouldn't people expect more from developers? Aren't we supposed to take a professional, empirical perspective on our practice? Shouldn't we be observing what works and what doesn't, and continually revising our opinions?
Sadly, over the many decades of our professional lives, we've rarely known this to be the case. Most engineers just seemed to know they were right; they pounced on data when it confirmed their biases and ignored it when it didn't.

## Call for Submissions

Do you have a surprising result or industrial experience? Something that challenges decades of conventional thinking in software engineering? If so, email a one-paragraph synopsis to timm@ieee.org (use the subject line "REDIRECTIONS: Idea: your idea"). If that looks interesting, I'll ask you to submit a 1,000-to 2,400word article (in which each figure or table counts as 250 words) for review for the Redirections department. Please note: heresies are more than welcome (if supported by well-reasoned industrial experiences, case studies, or other empirical results).-Tim Menzies "Well, hang on," you might say. "Isn't this just your own bias speaking? Haven't you just been remembering those few nasty developers who were stubborn, opinionated so-and-so's and conveniently forgetting the virtuous majority of developers who are open-minded, always learning, and thoroughly evidence-based?"
Well, we always wondered that ourselves. So, we went meta and got all evidence-based regarding this, our own long-held, "self-evident" belief.

## Gathering the Evidence

During the summer of 2015, Prem Devanbu visited Christian Bird and Thomas Zimmermann at Microsoft Research's Empirical Software Engineering group. When discussing possible summer projects, Devanbu pitched this question as a subject worthy of investigation: Wouldn't Microsoft want its developers' practice to be evidence-based? If its developers' beliefs weren't evidencebased, then where did these beliefs come from? Did these beliefs arise from, and closely correspond with, actual data from the developers' projects?
In the following, we summarize the results of investigating these questions. For more details, see the full paper. (Devanbu, "Belief & Evidence in Empirical Software Engineering") Early on, we had a series of discussions on how to explore these questions. We wanted to find out what people believed, why they believed it, and whether those beliefs corresponded with reality. We decided to pursue a triangulated 3 experimental method, combining a survey with an observational study.
First, we designed the survey and selected the target audience. We aimed to compile a range of propositionsstatements on which the respondents could agree or disagree. In particular, we wanted propositions on which developers were likely to have informed opinions. That is, they were likely to have encountered pertinent evidence during their engineering practice. We also wanted to choose propositions for which we could actually gather evidencethat is, deploy an observational study based purely on archived project data.
Our survey included these types of propositions:
• Code quality (defect occurrence) depends on which programming language is used.
• Fixing defects is risker (more likely to cause future defects) than adding new features. • Geographically distributed teams produce code whose quality is just as good as that of teams that aren't geographically distributed. • When it comes to producing code with fewer defects, specific experience in the project matters more than overall programming experience. • Stronger code ownership (fewer people owning a module or a file) leads to better code quality. • Merge commits are buggier than other commits. • Components with more unit tests have fewer customer-found defects. • More defects are found in more complex code. • Using assertions improves code quality. • Using static-analysis tools improves quality. • Coding standards help improve software quality. • Code reviews improve software quality.
We were also interested where these opinions came from and how strongly the respondents held them.
To that end, we asked the respondents to indicate the strength of their belief in each proposition, using a simple 5-point Likert scale (from strongly disagree to strongly agree). We then had the respondents select from six possible origins (Personal Experience, Peer Opinion, Mentor or Manager, Trade Journal, Research Paper, and Other) of their opinion and rank those origins by importance.
We also requested demographic data, including the respondents' age, gender, number of years as a developer, number of years at Microsoft, title at Microsoft, geographic location, and highest level of schooling, and whether they had a supervisory role. To identify relevant repositories (for the corroborating observational studies), we also asked for the name of the project and the organizational (high-level) division. We maintained the respondents' anonymity throughout. We sent the survey to approximately 2,500 Microsoft developers; we received 564 responses-a response rate of approximately 23 percent.

## And the Survey Said …

Unfortunately, there isn't enough space here to reproduce the full story. One area we won't go into much here is the level of disagreement between the respondents. The propositions listed earlier are arranged by the level of controversy. The first proposition aroused the most controversy; the last one virtually none. For more details, see the full paper. (Devanbu, "Belief & Evidence in Empirical Software Engineering") Here, we focus on two major issues that gave us concern: the opinion's source and whether developers' opinions were related to the observable phenomena in their projects.

## The Opinion's Source

As we mentioned before, the developers chose the primary sources related to each proposition and ranked them. So, for each question-developer pair, we had a set of ranks, of cardinality 6, with values from 0 (lowest) to 5 (highest).
Figure 1 illustrates the results. Personal Experience ranked highest, then Peer Opinion, Mentor or Manager, Trade Journals, Research Papers, and Other. We found no significant differences in these results across the demographic categories.
These results weren't that surprising; earlier research in social science has found much the same thing: opinion influence preferentially flows along stronger social ties. (Brown, "Social Ties and Word-of-Mouth Referral Behavior") But think on that a minute. Really? We, as empirical software engineering researchers, found this profoundly distressing. Our life's work, embodied in research papers, counted so little toward forming the opinions of professional practitioners at one of the world's leading software companies?
Look at it another way. Suppose a doctor recommended some powerful opioid for a minor (but annoying) digestive problem. Then, when asked what the research was on the drug's effectiveness, he said, "Ah, I don't know about research! It just works well in my personal experience; trust me." That wouldn't be very reassuring.
However, these days, medicine offers an inspiring model. Evidence-based medicine (EBM) is relatively new, (Unknown, "Evidence-Based Medicine: A New Approach to Teaching the Practice of Medicine") dating back to as recently as the early 1990s. Since then, EBM has caused a revolution in medical practice. Closer to home, Barbara Kitchenham and others have been advocating for evidence-based software engineering since 2004. (Kitchenham, "Evidence-Based Software Engineering and Systematic Reviews") Perhaps we will soon catch up to medicine.

## The Relationship between Opinions and Evidence

For this investigation, we chose a highly controversial proposition: Geographically distributed teams produce code whose quality (defect occurrence) is just as good as that of teams that are not geographically distributed.
We looked at two projects, which we'll call A and B. In the survey responses, developers working on A largely and strongly disagreed with this proposition, whereas developers working on B largely and strongly agreed with it. Surprisingly, both projects were widely distributed. Both had approximately 8,000 developers distributed across 100 buildings, and the repositories for both projects received commits from multiple buildings, campuses, and countries.
To quantify that distribution, we considered the percentage of files in each project that received at least 75 percent of their commits from one geographic area: building, city, region (for example, time zone), and country. The higher this percentage, the less distributed the development was. In project A, the percentages were 56 for building, 90 for city, 91 for region, and 92 for country. For B, the corresponding percentages were 76 for building, 80 for city, 83 for region, and 85 for country. Thus, B was slightly more distributed.
We then gathered defect data from both projects, based on the number of defect repairs in each file. We also gathered data per file on confounding predictive factors known to affect software quality, such as size, ownership, committer count, and commit count. Finally, we included a binary variable for each file's distribution, at each distribution level (building, city, and so on), indicating whether at least 75 percent of the commits to a file rolled up to that distribution level. After much careful modeling, (Devanbu, "Belief & Evidence in Empirical Software Engineering") we found that the data supported some interesting interpretations. The survey respondents ranked the importance of their opinions' sources, quoted from our full paper. (Devanbu, "Belief & Evidence in Empirical Software Engineering") We found no significant differences in these results across the demographic categories.
First, in both projects, when we controlled for the confounding factors, the effect of distribution was statistically significant but very, very small. Prior research has found much the same situation. (Bird, "Does Distributed Development Affect Software Quality? An Empirical Case Study of Windows Vista")(Kocaguneli, "PURPOSE: The IEEE Computer Society is the world's largest association of computing professionals and is the leading provider of technical information in the field. MEMBERSHIP: M.") The effects were statistically significant only because the sample sizes were large (around half a million files). In most cases, the effect size was a small fraction of what's even considered a small effect. In both projects, it appeared to be very slightly better (for quality) to be in the same building; in project B, it appeared to be only slightly better to be in the same building. Notably, in all other cases, it appeared to be very slightly (but statistically) better to be in different regions or countries. Thus, it appeared that in Project B, developers' belief was consistent with the observable data, whereas in project A, it was the opposite.
We interpreted these findings as consistent with the survey. The developers' strong, prevalent opinions indeed appeared to be largely subjectively formed, rather than arising from the rigorous, evidence-based reasoning we'd like to see from professionals.
On one hand, these findings are distressing to researchers. We'd like to think our work is having an impact on development practice. Apparently, it isn't. On the other hand, we take these findings as inspiration. Empirical studies of software engineering are progressing in leaps and bounds, thanks to substantial investment in data gathering and analysis in some organizations, and thanks to great interest in academic circles due to the public availability of open source data. The rapidly growing body of people, ideas, and results suggests that much better times are ahead. Academics are introducing curricula that present the empirical perspective early and often in software engineering courses. Practitioner magazines are increasingly paying attention. More and more organizations are getting savvy about data-based process improvement.
W e hope that our re- search serves as a starting point, a rallying cry, to get software engineering practice more data-driven and evidencebased, and thus more effective in producing better software, cheaper and faster.
What do you think? Do you know of ways to increase the impact of empirical research in software engineering? Then join the conversation.
Write the next article for the Redirections department-after all, this is your community.
