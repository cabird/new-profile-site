# AI Where It Matters: Where, Why, and How Developers Want AI Support in Daily Work

**Abstract**

Generative AI is reshaping software work, yet we lack clear guidance on where developers most need and want support, and how to design it responsibly. We report a large-scale, mixed-methods study of N=860 developers that examines where, why, and how they seek or limit AI help, providing the first task-aware, empirically validated mapping from developers' perceptions of their tasks to AI adoption patterns and responsible AI priorities. Using cognitive appraisal theory, we show that task evaluations predict openness to and use of AI, revealing distinct patterns: strong current use and a desire for improvement in core work (e.g., coding, testing); high demand to reduce toil (e.g., documentation, operations); and clear limits for identity-and relationship-centric work (e.g., mentoring). Priorities for responsible AI support vary by context: reliability and security for systems-facing tasks; transparency, alignment, and steerability to maintain control; and fairness and inclusiveness for human-facing work. Our results offer concrete, contextual guidance for delivering AI where it matters to developers and their work.

## Introduction

Developers increasingly work with generative AI tools (e.g., Copilot, Cursor) that promise faster delivery and lower cognitive load (Bird, "Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools")(Mckinsey, "Unleashing developer productivity with generative AI")(Overflow, "Stack Overflow Developer Survey")(Vaithilingam, "Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models")]. Yet adoption in software engineering (SE) reveals a persistent tension. Capabilities are advancing quickly (Hou, "Large language models for software engineering: A systematic literature review"), while integration often proceeds without a clear view of where developers need help, where they prefer to retain control, and how to design for responsible support (Bird, "Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools")(Choudhuri, "What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI")(Russo, "Navigating the complexity of generative AI adoption in software engineering"). Without this clarity, automation risks optimizing the wrong aspects of SE work. Industry research highlights a paradox in which developers who use AI report higher satisfaction and more time in a "flow state," yet spend less time on work they consider valuable, which can weaken professional identity and the quality judgments that define effective software work (Storer, "How gen AI affects the value of development work").
In this paper, we use "AI" to refer specifically to developer tools powered by generative AI models, including commercial offerings like GitHub Copilot, Claude, and Cursor, as well as bespoke in-house solutions that assist with various aspects of software development.
We argue that meaningful AI integration requires understanding how developers themselves evaluate and experience different aspects of their work. Prior research has documented AI adoption patterns and identified task-level preferences (Khemka, "Toward Effective AI Support for Developers: A survey of desires and concerns")(Vaz Pereira, "Exploring GenAI in Software Development: Insights from a Case Study in a Large Brazilian Company")(Russo, "Navigating the complexity of generative AI adoption in software engineering"). For example, studies have shown that factors like workflow fit can outweigh perceived usefulness in early adoption (Russo, "Navigating the complexity of generative AI adoption in software engineering"), and have begun to differentiate AI receptivity across tasks such as coding, testing, and documentation (Khemka, "Toward Effective AI Support for Developers: A survey of desires and concerns")(Lambiase, "Exploring Individual Factors in the Adoption of LLMs for Specific Software Engineering Tasks")(Vaz Pereira, "Exploring GenAI in Software Development: Insights from a Case Study in a Large Brazilian Company"). Other work has highlighted the gap between developers' ideal and actual workweeks, identifying toil-heavy activities as prime candidates for AI support (Kumar, "Time Warp: The Gap Between Developers' Ideal vs Actual Workweeks in an AI-Driven Era").
While these studies explain which tasks developers want automated, they do not provide accounts for where, why, and how developers seek or limit AI for different aspects of SE work.
To address this gap, we apply cognitive appraisal and work design theories to capture how developers assess tasks along dimensions of relevance, identity congruence, accountability, and cognitive demands (Folkman, "Dynamics of a Stressful Encounter: Cognitive Appraisal, Coping, and Encounter Outcomes")(Stephen E Humphrey, "Integrating motivational, social, and contextual work design features: a metaanalytic summary and theoretical extension of the work design literature")(Lips, "Discriminating between 'meaningful work'and the 'management of meaning"). Such appraisals shape not only task engagement but also openness to external support, making them critical for understanding where AI complements developer workflows (De Freitas, "Psychological factors underlying attitudes toward AI tools").
We present findings from a large-scale mixed-methods study of 860 software developers at Microsoft, examining how task appraisals predict AI adoption (where/why) and which design principles developers prioritize for responsible AI integration in SE (how).
Our investigation addresses two research questions (RQs):
‚Ä¢ RQ1: How do developers' task appraisals shape their openness to and use of AI tools? Where and why do they seek or limit AI support? ‚Ä¢ RQ2: Which Responsible AI (RAI) design principles do developers prioritize for AI support in SE tasks, and how do these priorities vary with experience and AI dispositions? Using quantitative ratings across SE task categories and thematic analysis of rationales, combined with forced-choice prioritization of RAI principles, we map current need/usage patterns and the underlying psychological and professional considerations shaping them.
Our findings reveal distinct clusters of SE tasks that differ in their suitability for AI support. We present a quadrant map that compares support needs with current use, highlighting gaps between developer preferences and available tools, and identifies opportunities for targeted tool development. We also find that trust requirements depend on context, with system-facing work requiring stronger reliability and transparency than exploratory or creative work. Taken together, these results support a framework for calibrating AI assistance that preserves developer agency, fosters expertise, and sustains meaningful work-delivering AI where it matters to developers.

## Related Work

As AI tools enter development workflows, understanding what drives developers to adopt or resist these tools has become a focal topic in SE research (Bird, "Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools")(Choudhuri, "What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI")(Lambiase, "Exploring Individual Factors in the Adoption of LLMs for Specific Software Engineering Tasks")(Russo, "Navigating the complexity of generative AI adoption in software engineering"). Prior work has applied technologyacceptance models (e.g., UTAUT (Venkatesh, "User acceptance of information technology: Toward a unified view")) to understand AI adoption, finding evidence that workflow compatibility and habitual use outweigh traditional factors such as performance or effort expectancy (Russo, "Navigating the complexity of generative AI adoption in software engineering"). Trust has also emerged as a key factor, shaped by tooling capabilities, user dispositions (e.g., risk tolerance, technophilia), and expectations of control (Butler, "Dear Diary: A randomized controlled trial of Generative AI coding tools in the workplace")(Choudhuri, "What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI")(Johnson, "Make Your Tools Sparkle with Trust: The PICSE Framework for Trust in Software Tools. In 2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering.").
More recently, studies have shifted from general adoption to investigating task-level differences (Khemka, "Toward Effective AI Support for Developers: A survey of desires and concerns")(Kumar, "Time Warp: The Gap Between Developers' Ideal vs Actual Workweeks in an AI-Driven Era")(Vaz Pereira, "Exploring GenAI in Software Development: Insights from a Case Study in a Large Brazilian Company"). Lambiase et al. (Lambiase, "Exploring Individual Factors in the Adoption of LLMs for Specific Software Engineering Tasks") show that AI receptivity is higher for artifact manipulation and information-retrieval tasks, but lower in collaborative contexts. In SE, Pereira et al. (Vaz Pereira, "Exploring GenAI in Software Development: Insights from a Case Study in a Large Brazilian Company") observes stronger adoption patterns for codeintensive work, with limited use in creative aspects. Khemka and Houck (Khemka, "Toward Effective AI Support for Developers: A survey of desires and concerns") report strong demand for AI in testing, debugging, documentation, and compliance, though these preferences were tempered with concerns about AI gimmickry and defects. Complementing these findings, Kumar et al. (Kumar, "Time Warp: The Gap Between Developers' Ideal vs Actual Workweeks in an AI-Driven Era") compares developers' ideal versus actual time allocations in daily work and show that toil-heavy activities (e.g., documentation, environment setup) correlate with reduced satisfaction and productivity. These tasks are disproportionately seen as work to minimize, positioning them as strong candidates for AI support. Collectively, this literature indicates that AI adoption is not a monolith; it is calibrated to the nature of the task. Yet it stops short of probing the psychological rationales that shape delegability. For example, why does coding count as "ideal" time, while infrastructure work or rote refactoring does not?
Our study addresses this gap by shifting from solely a capability/fit perspective to a meaning-based account: developers ask not only "Can AI do this?" but also "Should it?" and "To what extent?" We examine how developers cognitively appraise various aspects of their SE work and use that to explain where, why, and how they seek or limit AI (see ¬ß5.2). This perspective shows where human oversight and control remain essential even when AI is used.
Additionally, to our knowledge, this is the first study to examine developers' task-conditioned priorities for Responsible AI (RAI) principles in AI-powered SE tools (see ¬ß5). We investigate how they want these tools designed-specifically, which RAI features they prioritize for responsible support across SE tasks. Finally, we show priorities vary by SE/AI experience and individual AI dispositions to guide adaptive, task-and user-sensitive design.

## Appraisal Foundations & Hypotheses

Individuals are meaning-makers; we actively seek significance and value in our experiences (Lips, "Discriminating between 'meaningful work'and the 'management of meaning"). At work, we implicitly evaluate tasks by asking: Is this important to me? Does this align with what I want to do? Am I responsible if it fails? Can I handle its demands? Cognitive appraisal theory (Richard, "Emotion and adaptation")(Ira, "Appraisal theory. Appraisal processes in emotion: Theory, methods, research") formalizes these judgments across dimensions of relevance/importance, congruence with one's motivations or identity, accountability, and cognitive demands. These appraisals shape coping strategies (Tavis S Campbell, "Cognitive appraisal") and predict downstream outcomes such as engagement, persistence, and discretionary effort (John, "A three-component conceptualization of organizational commitment"). Complementing this, decades of work-design research (Stephen E Humphrey, "Integrating motivational, social, and contextual work design features: a metaanalytic summary and theoretical extension of the work design literature")(Lips, "Discriminating between 'meaningful work'and the 'management of meaning") show that job characteristics cluster into motivational (value, enjoyment), social (responsibility), and contextual (workload) factors, explaining substantial variance in work satisfaction and productivity (Stephen E Humphrey, "Integrating motivational, social, and contextual work design features: a metaanalytic summary and theoretical extension of the work design literature").
At this intersection, we focus on four appraisal drivers: Value, Identity, Accountability, and Demands. Value and Identity capture motivational aspects that make tasks meaningful (Lips, "Discriminating between 'meaningful work'and the 'management of meaning"); Accountability reflects the social stakes of responsibility (Philip E Tetlock, "Accountability and complexity of thought"); and Demands index the contextual difficulty and cognitive effort involved (Arnold, "The job demands-resources model: State of the art"). These drivers shape how individuals perceive ownership, risk, and burden (Bailey, "A review of the empirical literature on meaningful work: Progress and research agenda")(Arnold, "The job demands-resources model: State of the art")(Koestner, "Attaining personal goals: self-concordance plus implementation intentions equals success"), thereby influencing whether, when, and to what extent they seek support (Lubars, "Ask not what AI can do, but what AI should do: Towards a framework of task delegability")(Parasuraman, "A model for types and levels of human interaction with automation")(Shneiderman, "Human-centered artificial intelligence: Reliable, safe & trustworthy"). . In SE, we hypothesize that developers' openness to and use of AI are shaped by these drivers:
Value is the perceived importance of a task, i.e., its significance to project success, stakeholders, or personal goals (Hackman, "Motivation through the design of work: Test of a theory"). It contributes to a belief system that one's work matters (Blake A Allan, "Outcomes of meaningful work: A meta-analysis")(Lips-Wiersma, "Why we don't talk about meaning at work"). Accordingly, highvalue tasks heighten attention, focus, and satisfaction, but also raise anxiety about failure (Bailey, "A review of the empirical literature on meaningful work: Progress and research agenda"). Historically, such tasks attract tooling support, provided reliability is high (Parasuraman, "A model for types and levels of human interaction with automation"). In SE, this tension could mean that developers welcome AI assistance to increase efficiency, yet hesitate to cede too much control in core aspects.
H1. Higher task value increases developers' openness to AI support and usage. We expect that developers seek AI support as a means to complement meaningful tasks, rather than replacing them outright.
Identity alignment is the degree to which a task reflects one's interests, expertise, or professional self-concept (Hackman, "Motivation through the design of work: Test of a theory")(Richard, "Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being"). Such tasks are intrinsically motivating and foster a sense of authenticity, purpose, and ownership (William A Kahn, "Psychological conditions of personal engagement and disengagement at work")(Koestner, "Attaining personal goals: self-concordance plus implementation intentions equals success"), which can heighten reluctance to delegate them to AI (Lubars, "Ask not what AI can do, but what AI should do: Towards a framework of task delegability"). Yet, identity can also increase engagement with tools that help enact or amplify one's craft (Shneiderman, "Human-centered artificial intelligence: Reliable, safe & trustworthy"). Developers may therefore resist ceding identity-defining work, while strategically using AI to explore or extend their capabilities. H2. Higher task identity reduces developers' openness to AI support, but can increase usage when AI serves to complement expertise.
Accountability refers to the degree of perceived responsibility and potential blame an individual feels for a task's outcome (Jennifer, "Accounting for the effects of accountability")(Philip E Tetlock, "Accountability and complexity of thought"). High-accountability tasks are those where errors carry serious reputational or organizational consequences (e.g., customer-facing failures). Accountability Theory (Philip E Tetlock, "Accountability and complexity of thought") suggests that when individuals anticipate evaluation or social recognition, they become more deliberate and information-seeking, often turning to external aids as safeguards against errors (Lubars, "Ask not what AI can do, but what AI should do: Towards a framework of task delegability") and decisions (Hall, "An accountability account: A review and synthesis of the theoretical and empirical research on felt accountability")(Jennifer, "Accounting for the effects of accountability"). This could mean, rather than avoiding AI, developers strategically use it to substantiate contributions in high-stakes tasks.
H3. Higher task accountability increases developers' openness to AI support and usage. At the same time, accountability lowers tolerance for automation bias (Parasuraman, "Complacency and bias in human use of automation: An attentional integration")(Parasuraman, "A model for types and levels of human interaction with automation"). Since mistakes ultimately fall on them, developers are likely to adopt a cautious stance, insisting on oversight and decision control.
Demands capture the cognitive effort and load a task imposes (Arnold, "The job demands-resources model: State of the art"). High-demand work strains coping resources, increasing receptivity to aids that reduce mental load (Richard, "Emotion and adaptation")(Sweller, "Cognitive load during problem solving: Effects on learning"). Developers may turn to AI to lower the cognitive cost of experimentation, delegate effort-intensive components, and sustain momentum in demanding work.
H4. Higher task demands increase developers' openness to AI support and usage.
Controls and Groups: We control for developers' SE and AI experience, as both can shape baseline attitudes toward AI (Bansal, "Does the whole exceed its parts? the effect of ai explanations on complementary team performance")(Crowston, "Deskilling and upskilling with AI systems"). Beyond expertise, individual dispositions can condition how task appraisals translate into AI use. Here, we emphasize risk tolerance and technophilia (Burnett, "GenderMag: A method for evaluating software's gender inclusiveness")-traits linked to stronger AI-adoption tendencies (Choudhuri, "What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI"). Risk-tolerant developers may delegate demanding work and feel less deterred by accountability pressures, while technophiles (intrinsically eager to experiment with tools) actively seek opportunities to integrate AI (Choudhuri, "What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI"). Accordingly, we expect these factors to moderate the hypothesized relationships.

## Method

To address our RQs, we surveyed software developers at Microsoft. Microsoft employs over 60,000 developers worldwide, spanning diverse domains, team structures, processes, and stakeholder contexts. This scale, combined with exposure to both mature and emerging AI tooling, makes it a rich and diverse setting for our study.

## Study Design

The goal of our study was to: (1) characterize how developers appraise SE tasks; (2) assess how these appraisals shape their openness to and use of AI; (3) identify opportunities and gaps where AI can better support developer workflows; and (4) understand which Responsible AI (RAI) design principles developers prioritize in AI tools to credibly support different aspects of SE work. The study was reviewed and approved by Microsoft's IRB.
Synthesizing a taxonomy of SE tasks: To study task appraisals (RQ1), we first constructed a representative, grounded taxonomy of SE tasks (Table 1), integrating multiple empirical sources (Choudhuri, "What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI")(Khemka, "Toward Effective AI Support for Developers: A survey of desires and concerns")(Kumar, "Time Warp: The Gap Between Developers' Ideal vs Actual Workweeks in an AI-Driven Era")(Andr√©, "Today was a good day: The daily life of software developers"). We first drew on recent work-week studies of developer activities (Kumar, "Time Warp: The Gap Between Developers' Ideal vs Actual Workweeks in an AI-Driven Era")(Andr√©, "Today was a good day: The daily life of software developers"), that provided detailed task inventories and their higher-level groupings. We then enriched this set with jobdistribution insights from large-scale developer surveys on AI trust and adoption (Choudhuri, "What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI")(Khemka, "Toward Effective AI Support for Developers: A survey of desires and concerns"), ensuring our taxonomy reflected SE responsibilities distributed across roles, geographies, and contexts. Finally, we triangulated coverage through pilot sessions with developers and SE researchers outside our team, identifying any missing tasks and validating the clarity of category boundaries.
Responsible AI (RAI) principles: To assess developers' RAI priorities in AI-enabled SE tools (RQ2), we anchored our study in Microsoft's Responsible AI framework (Unknown, "Responsible AI Principles and Approach"). This framework synthesizes established AI ethics and governance guidelines (Unknown, "Foundations of Trustworthy AI: Governed Data and AI, AI Ethics and an Open Diverse Ecosystem")(Unknown, "The Montreal Declaration for a Responsible Development of Artificial Intelligence")(S European Commission, "Ethics guidelines for trustworthy AI")(Google, "Artificial Intelligence at Google: Our Principles"), and includes: Reliability & Safety, Privacy & Security, AI Accountability (provenance), Fairness, Inclusiveness, and Transparency. We extended this set with Steerability (user agency/autonomy) and Goal maintenance (sustained alignment with user goals) principles, both centrally emphasized in recent RAI research (Choudhuri, "What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI")(Jakesch, "Saleema Amershi, and Alexandra Olteanu. 2022. How different groups prioritize ethical values for responsible AI")(Lynch, "Agentic Misalignment: How LLMs could be insider threats")(Tankelevitch, "The metacognitive demands and opportunities of generative AI"). This combined set provided a comprehensive basis for answering RQ2.
Survey design: We followed Kitchenham's guidelines for conducting surveys (Barbara, "Personal opinion surveys") and drew on established theoretical frameworks and validated instruments from behavioral sciences and Human-AI Interaction (HAI) research (Table 2). The survey was refined through iterative validation with external researchers, and multiple one-onone sandbox testing and pilot rounds.
Our final survey comprised three sections:
(1) AI experience and dispositions: After obtaining informed consent, we asked participants about their experience with AI tools and their dispositions toward its use in work. We prefaced this section with a standard description of developer-facing AI tools, adapted from the DORA 2025 survey (Research, "DORA Research: 2025 Overview"). Participants with no prior AI-tool experience exited the survey at this point. 1) that best reflected their current work and answered the subsequent questions for those categories. To reduce fatigue, the meta-work category (applicable to all developers) was excluded from the initial selection and shown only if a participant had selected two categories; thus, no participant completed more than three category blocks.
(3) Task Category blocks: Each task category was a separate block. For each selected category (e.g., Development, Design & Planning, Quality & Risk Management; see Table 1), participants answered:
(a) RQ1: Task appraisals and AI use. For each task in a category (e.g., Testing/QA, Security, and Code Review under Quality & Risk Management), participants rated task value, identity, accountability, and demands measured with validated instruments (Table 2). We used single-item measures to reduce participant fatigue, given these items retain psychometric validity for concrete, well-scoped constructs (Russell A Matthews, "Normalizing the use of single-item measures: Validation of the single-item compendium for organizational psychology"). Participants then reported their openness to AI support and frequency of AI use for each task (dependent variables for RQ1). Finally, we asked two open-ended questions: (a) where they most wanted AI support, and (b) where they preferred to limit it; within the task category (e.g., in Quality & Risk Management), and why. (b) RQ2-RAI priorities. Participants selected any five RAI principles (from the eight listed earlier) they deemed most important for AI-enabled tooling in that category (with the five-choice format drawn from prior work (Jakesch, "Saleema Amershi, and Alexandra Olteanu. 2022. How different groups prioritize ethical values for responsible AI")). This top-N design forces trade-offs and mitigates ceiling effects ("all-high" bias common in Likert importance ratings) (Unknown, "The reliability of survey attitude measurement: The influence of question and respondent attributes")(Norman M Bradburn, "Asking questions: the definitive guide to questionnaire design-for market research, political polls, and social and health questionnaires"). After selecting, participants could optionally describe experiences that made their choices salient for that category. We tested alternative elicitation formats (ranking, point allocation, MaxDiff, importance categorization) (Norman M Bradburn, "Asking questions: the definitive guide to questionnaire design-for market research, political polls, and social and health questionnaires") and chose this approach based on sandbox feedback.
Because RAI principles can be abstract and participants may not easily connect them to specific AI contexts (Cave, "Portrayals and perceptions of AI and why they matter"), we provided ondemand, plain-language explanations (adapted from (Jakesch, "Saleema Amershi, and Alexandra Olteanu. 2022. How different groups prioritize ethical values for responsible AI")), via information icons next to each principle. Each explanation followed a consistent format: (a) what a system embodying the principle would do, and (b) an example realizing its application, while retaining a degree of generality (see (Jakesch, "Saleema Amershi, and Alexandra Olteanu. 2022. How different groups prioritize ethical values for responsible AI")).
The survey concluded with an open-ended question inviting general comments on AI use at work, and an optional field to share an alias for follow-up contact. In the pilot, participants could also suggest tasks missing from the taxonomy (for the categories they answered) and/or provide general survey feedback.
We administered the survey in Qualtrics (Qualtrics, "Qualtrics Survey Platform"). All closed-ended questions used a 5-point Likert scale, and a sixth option ("I'm not sure" or "I don't do this task/N.A.") to distinguish ignorance from indifference (Grichting, "The meaning of "I Don't Know" in opinion surveys: Indifference versus ignorance"). The survey took 10-15 minutes to complete. To ensure data quality and reduce response bias, we included attention checks, randomized questions and option orders within each block, and randomized the order of task-category blocks. The complete survey instrument is provided in supplemental material. [1].
Sandbox and pilot: We sandboxed the survey one-on-one with developers and SE/HCI researchers (n=11) to assess its clarity, interpretability, and realism. Based on their feedback, we revised ambiguous questions, added safeguards against automated submissions, and contextualized questions to reflect participants' current work (e.g., a once-valuable task may no longer be relevant in current work). Initially, all participants saw the meta-work block (Table 1), but pilots showed that limiting each respondent to at most three category blocks improved data quality and reduced fatigue, so we updated the design. Additionally, we tested multiple elicitation formats for RAI prioritization (Norman M Bradburn, "Asking questions: the definitive guide to questionnaire design-for market research, political polls, and social and health questionnaires"). Respondents were comfortable selecting top-N principles and explaining tradeoffs, over ranking ethical values. We adopted this format consistent with prior work in this field (Jakesch, "Saleema Amershi, and Alexandra Olteanu. 2022. How different groups prioritize ethical values for responsible AI").
To finalize the survey, we piloted it with (n=50) developers. This validated the survey's clarity and task set coverage. Minor wording edits were made, and pilot responses were excluded from the analysis.

## Data Collection

Distribution: Following pilots, we distributed the survey to 8,000 software developers at Microsoft via email in July 2025. Developers were sampled uniformly at random across product groups, roles, and geographies, in accordance with internal survey policies. To incentivize participation, respondents could enter a raffle for ten $50 AmEx gift cards. One reminder email was sent after a week to boost response rates. Participation was voluntary; responses were anonymous unless participants opted in to follow-up contact.
Sample size: To determine the appropriate sample size, we conducted an a priori power analysis in G*Power (Faul, "Statistical power analyses using G* Power 3.1: Tests for correlation and regression analyses") for multiple linear regression with repeated measures, using the number of predictors in our design. We targeted the detection of even a small effect size (ùëë = 0.05) at a significance level of ùõº = 0.05 with power = 0.95. The analysis indicated a minimum of 245 responses. To accommodate missing data, quality exclusions, and subgroup analyses, we targeted at least three times this number.
Responses: We received 1,193 responses, a response rate of 14.86%, consistent with the response rates of prior SE surveys (Punter, "Conducting on-line surveys in software engineering")(Storey, "Towards a theory of software developer job satisfaction and perceived productivity"). We removed incomplete (ùëõ = 152) and patterned responses (straight-lined or repetitive altering; ùëõ = 59), as well as those that failed attention checks (ùëõ = 98) or reported no AI experience (ùëõ = 24). We considered "I'm not sure"/"I don't do this task" Likert selections as missing data.
We retained 860 valid responses from developers across six continents, representing a wide distribution of SE and AI experience. Most respondents were from North America (57.4%) and identified as men (73.8%), consistent with distributions reported in prior SE studies (Choudhuri, "What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI")(Russo, "Navigating the complexity of generative AI adoption in software engineering")(Trinkenreich, "A Model for Understanding and Reducing Developer Burnout"). A summary of participant demographics is available in the supplemental [1].

## Data Analysis

Quantitative: We analyzed data in Python and R to summarize distributions, fit regression models (see ¬ß 5.1,5.3), and generate visualizations. Closed-ended responses (Likert; Top-N) were visualized to assess variation in appraisals, openness to AI support, and usage across tasks (Tab. 4, Fig. 1) and RAI priorities across categories (Tab. 5, Fig. 2). For RQ1, the unit of analysis was (participant, task type); for RQ2, (participant, task category). Because the design involved repeated measures within participants and across tasks, we used mixed-effects regression (Gelman, "Data analysis using regression and multilevel/hierarchical models"). Full model specifications, diagnostics, and results are deferred to the corresponding subsections in ¬ß5; here we outline the overall approach and the units of analysis.
Qualitative: We used reflexive thematic analysis (Braun, "Using thematic analysis in psychology")(Braun, "Conceptual and design thinking for thematic analysis") to identify patterns in the data, iteratively refining them based on participants' responses (Braun, "Using thematic analysis in psychology"). To ensure rigor, the team held multiple meetings to compare codes, resolve differences, and build consensus, as recommended in thematic analysis (Braun, "Conceptual and design thinking for thematic analysis")(Creswell, "Qualitative inquiry and research design: Choosing among five approaches").
First, we inductively open-coded the data to capture preliminary ideas. We then refined and consolidated codes, merging conceptually similar ones while keeping others distinct, and linked them to relevant text segments. Throughout the process, we used a negotiated agreement protocol to guide team discussions until we reached consensus on the final themes (cataloged in [?]). Next, to understand why specific patterns emerged, we mapped qualitative insights to quantitative findings, again through consensus building. As an additional check, we compared participants' free-text responses with Likert selections and found no discrepancies between their assessments and explanations. Finally, where relevant, we triangulated findings with behavioral science theories to structure interpretation.
In total, we analyzed 1,528 responses about where developers seek and limit AI support and 2,453 responses explaining RAI-principle priorities, spanning five task categories. Participants are referenced as P1-P860 in subsequent sections.
We used member checking to validate our findings: results were sent to 371 participants who opted into follow-up contact, and 62 replied. Their feedback affirmed the findings and offered clarifications; no new insights or disagreements emerged.

## Results

In this section, we report (1) how task appraisals shape AI adoption (RQ1a: 5.1), (2) where developers seek or limit AI support (RQ1b: 5.2), and (3) which Responsible AI principles they prioritize in AI tools to credibly support their workflows (RQ2: 5.3).

## RQ1a: How do appraisals shape openness to and use of AI support?

To answer RQ1, we first investigated whether task appraisals (value, identity, accountability, demands) predict developers' (a) openness to AI support and (b) AI usage, and whether these relationships vary by developer characteristics (experience, AI dispositions).
For each outcome, we fit linear mixed-effects regressions (Gelman, "Data analysis using regression and multilevel/hierarchical models"), with appraisals as fixed effects; controls for developers' SE and AI experience, and random effects for participant and task type to capture within-person and across-task dependence. Models were estimated for the full sample (Tab. 3) and, per our planned group analyses, stratified by risk tolerance and technophilic motivations (see ¬ß 3). Group analyses statistics are in supplemental [1].
We checked for multicollinearity among the predictors before examining the results. All Variance Inflation Factors (VIFs) were < 2, well below the accepted cutoff of 5 (Joseph, "Multivariate data analysis"). We controlled false discovery rates (FDR) using the Benjamini-Hochberg procedure (Thissen, "Quick and easy implementation of the Benjamini-Hochberg procedure for controlling the false positive rate in multiple comparisons") and report results significant at ùõº = .05 after this correction.
Table 3 summarizes the regression results. All hypothesized effects (H1-H4, ¬ß3) were supported: each appraisal dimension significantly predicted both developers' openness to and use of AI support in work. We report marginal (ùëÖ 2 ùëö ; variance explained by fixed effects) and conditional (ùëÖ 2 ùëê ; variance explained by fixed and random effects) fit indices as indicators of model fit (Joseph, "Multivariate data analysis"). Task Value (H1) positively predicted openness to and use of AI support. A one-standard deviation (SD) increase in perceived task value raised openness by .12 and use by .16 SD units (p < 0.001, FDRcorrected), with medium effects (.16, .18) holding other factors constant (Table 3). In short, when developers viewed a task as important, they were more likely to use AI for efficiency (e.g., automating rote steps, comprehension, collaboration, information retrieval). They, nonetheless, stressed retaining decision control, positioning AI as complementary rather than substitutive (see ¬ß5.2).
Task Identity (H2) alignment showed a dual pattern: lower openness to AI support (ùõΩ=-.09, p<.001) but higher usage (ùõΩ=.15, p<.001), with medium effects (-.15, .20). Developers protected ownership of identity-defining work (see ¬ß5.2); yet used AI to refine their craft (e.g., learning, research, and exploration).
Task Accountability (H3) was positively associated with openness to AI support (ùõΩ = .07, ùëù < .001) and use (ùõΩ = .18, ùëù < .001), with small-medium effects (.10, .21). Rather than avoiding AI, developers leveraged it as a safeguard in high-stakes tasks (e.g., to surface issues, verify solutions, or justify decisions) raising both support needs and use. Yet, heightened accountability increased vigilance: they insisted on deliberate review of AI outputs, maintained oversight, and decision control for these tasks (see ¬ß5.2).
Finally, Task Demands (H4) positively associated with openness (ùõΩ=.12, p<.001) and use (ùõΩ=.09, p<.001), with small-medium effects (.18, .10). For demanding/effort-intensive work, developers were more inclined to use AI to offload rote steps, lower cognitive load, and sustain momentum. In these cases, AI functioned as a cognitive scaffold that freed attention for higher-order knowledge work (Lee, "The impact of generative AI on critical thinking: Self-reported reductions in cognitive effort and confidence effects from a survey of knowledge workers").
Experience: SE experience predicted lower AI use (ùõΩ = -.09, p<.001). Experienced developers rely on established repertoires (Anders, "The influence of experience and deliberate practice on the development of superior expert performance. The Cambridge handbook of expertise and expert performance"), reducing the perceived utility of AI delegation, whereas juniors use AI to offset skill gaps (Crowston, "Deskilling and upskilling with AI systems"). Openness to AI support did not differ significantly by SE experience. Prior AI experience increased both openness and use (ùõΩ = .19, .41; both p<.001), consistent with familiarity-driven calibration of expectations and AI-usage habits (Bansal, "Does the whole exceed its parts? the effect of ai explanations on complementary team performance").
Group analysis (AI Dispositions): Stratifying by median splits on reported AI dispositions, risk-tolerant (RT) developers showed higher openness and use overall. They sought significantly more AI support for high-value (ŒîùõΩ = .06, ùëù = .035) and high-demand (ŒîùõΩ = .09, ùëù = .001) tasks, and used more in high-stakes and demanding situations (Accountability: ŒîùõΩ = .07, ùëù = .038; Demands: ŒîùõΩ = .08, ùëù = .002). Risk-averse (RA) peers remained more vigilant under accountability pressures (Jennifer, "Accounting for the effects of accountability")(Philip E Tetlock, "Accountability and complexity of thought"). Other associations were consistent across both groups. Technophiles, likewise, showed higher openness and use overall. Crucially, accountability appraisals (H3) predicted these outcomes only among high-technophiles (Support: ùõΩ = 0.07, ùëù < .001; Usage: ùõΩ = 0.20, ùëù < .001), indicating that technophily moderates AI adoption under high-stakes conditions. Consistent with human-AI teaming work (Bansal, "Does the whole exceed its parts? the effect of ai explanations on complementary team performance"), high-technophiles have the orchestration habits to use AI as a "second set of eyes," which outweigh perceived error/coordination costs, whereas lowtechnophiles-lacking these routines-view AI as net-costly under accountability pressure. Other associations were comparable across both groups (no LT-HT differences; effects significant within each).
Takeaway: Task appraisals shape AI adoption: Value, Accountability, and Demands increase openness and use; Identityalignment shows dual effects. Junior, AI-experienced, risktolerant, and technophilic developers are more receptive overall, especially for high-value, high-stakes, or demanding work.

## RQ1b:

Where and why do developers seek or limit AI support?
Given that appraisals predict AI use, we examined how it varied across tasks to locate where and why developers seek or limit support.
First, we clustered tasks by their appraisal signatures (Table 4). For each task, we computed top-2 agreement proportion (share selecting 4-5 on a 5-point scale (Ladhari, "Developing e-service quality scales: A literature review")) for the four appraisals, then standardized these values to z-scores (ùëß = (ùë•x)/sd(ùë•)) for cross-scale comparability (Joseph, "Multivariate data analysis"). We applied agglomerative hierarchical clustering (Ward linkage) (Joe, "Hierarchical grouping to optimize an objective function") on Euclidean distances of these z-scores, selecting ùëò = 3 optimal clusters via silhouette analysis (Peter, "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis") (see [?] for silhouette plot). We used precision-weighting (inverse-variance shrinkage to the grand mean) to address unequal task-Ns and validated cluster stability via stratified bootstraps (B=1000) (Joseph, "Multivariate data analysis"). The analysis yielded the following clusters:
‚Ä¢ C1: Core work -High value and demands; moderate-high accountability; moderate-strong identity alignment. ‚Ä¢ C2: People & AI-building -Moderate value, demands, and accountability; strong identity alignment. ‚Ä¢ C3: Ops & Coordination -Moderate-high value, demands, and accountability; weak identity alignment. We simultaneously mapped tasks onto an Openness to AI Support (x) vs. AI Usage (y) plane (Fig. 1) to visualize gaps in tooling support. Axes show task-level z-scores for openness "need" (x) and reported use (y), with positive values above the sample mean and vice versa. Quadrants (mean-split: z=0) highlight distinct opportunities/gaps: ‚Ä¢ Build (bottom-right; high need, low use): Clear need but limited adoption; reduce friction and prototype new support. ‚Ä¢ Improve (top-right; high need, high use): Strong need and adoption; focus on reliability and quality for gains. ‚Ä¢ Sustain (top-left; low need, high use): AI is used but not essential; maintain support without over-investment. ‚Ä¢ De-prioritize (bottom-left; low need, low use): Limited uptake; expect lower returns from additional investment.
In what follows, we position tasks on this map and, by cluster, draw on qualitative analysis of free-text responses to explain which aspects may benefit (or suffer) from AI involvement and why.

## 5.2.1

Core work (C1) comprised tasks central to development and systemic quality-management: coding, bug fixing, testing/QA, code review, system design, performance optimization, requirements engineering, security; alongside learning and research. Appraised as highvalue, high-stakes, and high-demand, most C1 tasks concentrated in the Build/Improve (high-need) zones, indicating a strong appetite for AI support. Identity alignment, however, constrained delegation: participants sought AI primarily as an augmentation support while retaining ownership of core decisions, skills, and responsibilities. In contrast, system design and requirements fell in De-prioritize, due to AI's contextual misfit and trust concerns.
Seek AI: For core work, participants used AI to boost workflow efficiency, delegating tedious steps to reduce cognitive load: "generate boilerplate code, build configurations, test cases...which I know how to write, but I don't want to write" (P353); freeing focus for creative problem solving: "Leave me to do the fun [parts]" (P319). They sought proactive performance and quality assurance, beyond standards enforcement, to catch "bugs, regressions, [performance] bottlenecks, and potential security issues early...where human review might miss patterns or edge cases" (P241). This required multi-and cross-context awareness-AI that integrates signals across codebases, documents, and related artefacts: "It needs to look at logs, performance counters, etc., understand runtime behavior...then make changes, inspect results, try again" (P195). As P201 noted, "We want/need AI to do a MUCH better job of analyzing a current code base/architecture so it can understand how/where to add/extend. " (P201). Overall, they envisioned AI as a collaborator, aiding comprehension and pair programming/debugging/testing/review; without overriding human judgment: "The focus should be on AI making ME better at my job" (P213). For learning/research, participants wanted AI as a personalized guide, a tool for information retrieval/synthesis, and a scaffold for ideation and practice. They stressed adaptive help aligned with skills and goals: "it should build my concepts from the ground up...based on my learning styles" (P281); "walk me through tutorials, provide examples, point me to resources" (P169). They wanted AI to surface relevant material across sources: "Researching could be where AI plays a big role...it can easily pick out relevant information from large amounts of data" (P28). At the same time, they cautioned against shortcutting experiential learning: avoiding cases where "AI does all the work and [they] miss a chance to learn by doing" (P66).
Limit AI: Participants resisted fully automating core work. They welcomed assistance but insisted on retaining oversight and decision control: "I can't fully delegate the final code review to AI-my approval puts my name on it" (P117). Others echoed, "deciding whether to ship something with limitations or communicate a risk to leadership requires context, experience, and intuition that AI can't fully grasp" (P301). They emphasized that AI should not absolve them of accountability, "I wouldn't want AI to handle final decision-making in high-stakes scenarios; responsibility should remain with experienced professionals" (P9), or reduce their role to passive overseers: "Should not turn the worker into a George Jetson" (P45).
They also resisted AI to preserve professional identity and craft: "I do not want AI to handle writing code for me. That's the part I enjoy and is the core of my work" (P110), and warned that overreliance risks deskilling: "intellectual offloading can result in errors that eventually no one understands" (P409). P16 captured the ethos: "AI should enhance human engineers' learning and development, not replace tasks that allow them to become better engineers" (P16).
Trust and quality concerns reinforced these limits. Respondents cited hallucinations, lack of transparency, and weak contextual reasoning as reasons to keep AI in a supporting role: "AI should not be the determining factor in how to solve quality problems...it frequently hallucinates with absolute certainty" (P17). They were wary of AI handling sensitive data: "I don't want AI to directly handle sensitive information, as I don't trust that information it sees stays truly secure" (P397). They also flagged maintainability risks and AIinduced technical debt: "AI-generated code is often not very readable or maintainable, which reduces long-term sustainability" (P149).
System design & requirements fell in De-prioritize. Participants reported low need/use due to both AI's capability gaps and its contextual misfit with the situational nature of these tasks: "These decisions require deep domain knowledge and long-term vision that only experienced engineers can provide" (P241). They also warned of homogenized, generic outputs as a risk to innovation: "AI's system design solutions bias toward old known solutions rather than a modern solution that solves the problem better" (P188). Here, respondents preferred human judgment and collaboration.
Takeaway: Developers seek AI as a collaborator on core work to boost efficiency and reduce cognitive load; redirecting focus on higher-order work (H1, H4), while retaining oversight and decision control in high-stakes, identity-laden aspects (H2, H3).
Takeaway: For identity-laden and interpersonal work, developers resist AI and retain ownership due to craft, relationships, and personal growth (H2). AI is, at best, peripheral.

## 5.2.3

Ops & Coordination (C3) covered (a) ops/maintenance toil ("run-the-systems"): DevOps/(CI/CD), environment setup, code main-tenance (refactoring/updates), infrastructure monitoring/alerts, documentation; and (b) coordination/support ("relational work") overhead: project planning/management, stakeholder communications, customer support. Appraisals were moderate-high on value, demands, accountability but low on identity. Reported need for AI was moderate, yet adoption lagged due to AI limitations, context fit, and trust concerns. In Fig. 1, "run-the-systems" tasks concentrate in Build, while "relational-work" sits in De-prioritize zone.
Seek AI: Participants wanted AI to reduce grunt C3 work (toil), provided it was reliable, deterministic, and context-aware.
For "run-the-systems", they sought an assistant for well-scoped, effort-heavy tasks (e.g., setup, maintenance, monitoring) that were low in creative value. Current tools were seen as limited: setups remain manual, pipelines fragile, alerts noisy, and documentation stale. Participants desired AI to provision environments and configurations, automate upgrades and migrations, maintain system health, and update documentation from code or design changes. CI/CD was a recurring pain point, with calls for AI to generate and repair pipelines, enforce quality checks, and run unattended deployments. As P261 put it: "AI should help in maintenance of services, making sure that the lights are kept on when the developers move on to new features. Keep systems healthy; where safe, triage and remediate known issues" (P261). They also emphasized cross-context awareness-linking telemetry and artifacts to infer, triage, and predict failures: "constantly analyze logs, metrics, and system behavior to identify deviations, triage root causes, and predict potential failures before they impact operations or users" (PID309). For large-scale refactoring (situated in the Improve zone), participants wanted architecture-aware changes applied safely across the codebase.
For "relational work", AI was welcome backstage for handling logistics: drafting updates, summarizing meetings, pulling context across threads, scheduling follow-ups-and for PM support (contextaware plans, dependency tracking, lower coordination overhead), while keeping strategy human-led. "AI should prioritize task by impact/dependencies...free time for creative/strategic thinking...plans must stay adaptive and human-gated" (P403). Net: AI was framed as a peripheral tool and remained largely de-prioritized for relational work, for reasons detailed next:
(Limit AI) Participants de-prioritized AI for "relational work", arguing that contextual intuition, empathy, and long-term vision aren't automatable. Strategic calls and ambiguous trade-offs should remain human: "I don't want AI to handle 'interpretation, ' drive product vision, or make strategic trade-off decisions" (PID3). For stakeholder communications, they emphasized authenticity & relationshipbuilding: "I don't want AI to handle stakeholder comms...these require empathy, trust-building, and nuanced understanding of human dynamics" (PID172); "Client communication needs personal touch. Summarizing meetings is one thing, but replacing real touch points is too much" (PID217). Customer support drew similar pushback: "As a customer, being forced to an AI is frustrating" (P41), with accuracy lapses seen as risking support quality, and brand damage. These, alongside AI's capability gaps (hallucinations, weak grounding, high prompt overhead), kept it backstage (summarizing, retrieving, scheduling) while "humans hit send" (P47).
By contrast, "run-the-systems" tasks met less resistance in principle, but faced trust, quality, and transparency concerns with (and/or absence of) current tools, that kept them in Build zone. Help was welcome only with determinism, verifiability, and human-gated change control: "Anything that touches prod stays behind human gates-no auto-deploys, no direct live changes, no publishing without supervision and transparency" (P165). Performance lapses in current tools fueled caution on large-scale refactors-"AI should not perform any large-scale refactoring" (PID182)-with even small edits requiring review to prevent regressions. Finally, participants warned against over-automation that erodes operational intuition: "Engineers still need to learn how things work... AI should guide, not replace, or leave juniors without a pathway to operational knowledge" (PID16).
Takeaway: Developers offload ops/coordination toil (H2, H4) only when AI is reliable, safe, and context-aware (H1, H3). Still, they resist over-automation that erodes intuition or adds debt. Relational work aspects are off-limits-empathy, intuition, and authenticity remain irreducibly human.

## RQ2: Which RAI design principles do developers prioritize in AI for SE tasks?

Recall that participants selected five of eight RAI principles (see ¬ß 4) for the task categories most relevant to their work; these choices reflect top priorities under forced trade-offs, not an absolute ranking or organizational stance.
As shown in Fig. 2, participants most frequently selected Reliability & Safety (85%), Privacy & Security (77%), Transparency (72%), Goal Maintenance (68%), AI Accountability (67%), and Steerability (67%) across categories. Fairness (32%) and Inclusiveness (32%) followed. Participants' explanations indicated that, given the current maturity of AI tools in SE, they prioritize pre-requisites that ensure correctness, reduce harm, and keep the system aligned and under control, before expecting credible support for broader humanist principles: "Surely all of them are important but at which stage? Right now, the basics aren't even done well, so those are [what] I selected" (P43).
To examine how RAI priorities varied, we fit logistic Generalized Linear Mixed Models (GLMMs) (Norman, "Approximate inference in generalized linear mixed models") per principle (Table 5). The models predicted whether a principle was prioritized (ùëÇùë¢ùë°ùëêùëúùëöùëí = 0/1) as a function of task categories, mean-centered SE & AI experience, risk tolerance, and technophilia, with random intercepts for participant to account for repeated measures.
The intercept (baseline) represents a developer with average SE/AI experience and AI-dispositions (relative to their peers) prioritizing a principle in AI support for development-heavy work. Table 5 reports odds ratios (ORs) relative to this baseline (OR>1 = higher odds and vice versa). For example, the baseline odds of prioritizing Privacy & Security in AI for development tasks were 8.19 (i.e., 8.19/(1 + 8.19) ‚âà89% probability). In quality/risk-management tasks, the odds increased by 1.91 (8.19 √ó 1.91 = 15.65, ‚âà94% probability).
Next, we integrate qualitative accounts to explain why these patterns emerged and how priorities shifted across SE work categories.
Interpretation guideline: These results reflect preferences under a forced-choice design; they neither prescribe policy nor imply that any RAI principle is optional. All remain essential. Read them as a pragmatic "order of operations" for current tools, varying by task category and individual dispositions, as detailed next.

## How do priorities vary across task categories?

(1) For systems-facing work (development, infrastructure/ops, quality & risk)-concentrated in the Build/Improve zones of Fig. 1participants imposed strict gating before AI could be trusted. Reliability & Safety (base odds: 18.15; ‚âà95%) and Privacy & Security (base odds: 8.19; ‚âà89%) were non-negotiable. Participants stressed that for these tasks, AI errors do not "net to zero"-they waste time, send teams down unproductive paths, and amplify risk: "AI MUST BE correct for it to be useful. Incorrect AI may as well be throwing spaghetti
Development Design & Planning Quality & Risk Meta-work Infra & Ops % participants selecting principle Participants next emphasized Transparency (base odds: 5.17; ‚âà84%), seeking clear explanations to verify assumptions, catch hallucinations, and justify (or revise) AI contributions. They tied this need to personal accountability and learning: "I feel accountable for my work, so I feel accountable for what the AI has done. I want it to explain why an action was taken. This also helps me grow as a developer" (P36).
To keep tools aligned with shifting objectives, they highlighted Goal Maintenance (base odds: 4.68; ‚âà82%), stressing that AI must adapt as goals evolve; currently it chases tangents and/or resurfaces stale context, forcing rework: "The things I need to be giving my attention to are changing all the time, so if AI could keep up with that via Goal Maintenance, that would be huge" (P791). In cases where drift occurs, participants prioritized control surfaces to redirect AI (Steerability, base odds: 3.65; ‚âà79%) and provenance to backtrack-/trace errors (AI Accountability, base odds: 3.03; ‚âà75%): "Lack of steerability & accountability makes it hard to use AI for tasks that require extensive time/detail...it's hard to put it back on track and/or tell where it went wrong...I often have to start new chats, which is frustrating, losing progress because the context is gone" (P495).
In quality & risk work, Fairness surfaced as contextually salient (OR = 1.07). Fairness matters because reviews and audits affect releases and defect attribution. Participants stressed the need for unbiased evaluation: "Fairness of PR review is hard for humans, so I want it to be a success metric for AI reviewers" (P196); "It is important to have unbiased AI for quality-related tasks to ensure the integrity of the work is not compromised" (P461). Some saw fairness as nested within reliability/safety; others raised a tension between fairness and privacy, noting that bias checks might require "exposing personal attributes the AI shouldn't know" (P476).
( 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 AI Where It Matters: Where, Why, and How Developers Want AI Support in Daily Work Conference'17, July 2017, Washington, DC, USA and stakeholder communications: "If AI updates documentation, it must ensure inclusiveness and fairness so content works for all customers" (P120); "It is key that AI is bias/prejudice free to maintain stakeholder relationships" (P195).
Other priorities were consistent with the baseline (e.g., Transparency for learning/research, Privacy for sensitive communications, Steerability, and AI Accountability for system design). In design/planning, however, participants downweighted Reliability when AI served as an ideation scaffold (OR = 0.49), and prioritized Goal Maintenance (OR = 1.45). When AI scaffolds creativity, adaptability can outweigh strict determinism: "Creativity of AI is important; I'm willing to tolerate errors" (P180). Participants valued AI's ability to surface options that spark innovation (even if imperfect), provided it stayed aligned with (evolving) objectives: "During planning, goals frequently change, so AI needs to keep up with that evolution. I'd also expect AI to bring in much more outside perspectives to synthesize a range of feedback" (P459).
Takeaway: In systems-facing work, Reliability and Privacy are central; next come Transparency, Goal Maintenance, Steerability, and AI Accountability. In design and human-facing tasks, Fairness and Inclusiveness are elevated. Developers relax Reliability for creative scaffolding and emphasize Goal Maintenance as needs evolve. Net: Get safety/security right; keep AI explainable, aligned, steerable, and accountable, & make outputs fair and inclusive.

## How do priorities vary by experience/AI dispositions?

Across individual differences, Steerability rose in priority with higher SE experience (OR = 1.21), AI experience (OR = 1.11), risk tolerance (OR = 1.13), and technophilia (OR = 1.28). Viewed through Self-Determination Theory (Edward, "The" what" and" why" of goal pursuits: Human needs and the self-determination of behavior")(Richard, "Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being"), this pattern reflects protection of autonomy and competence: experienced developers favored control that keeps AI actions interruptible and easy to correct, citing course-correction overheads-"Sometimes it spirals off... backtracking is harder than starting over" (P657). Risk-tolerant individuals treated steerability as a safeguard for rapid intervention. Participants also resisted modes (e.g., bulk edits) that could erode competence over time: "Multi-file edit modes feel to take away steerability...Yes, the developer gets the final say, but I've noticed it harms the engineer's skills over the long term more than it helps" (P754).
Experienced SEs prioritized Reliability & Safety (OR = 1.15), consistent with a sharper sense of downstream "automation surprises" (Parasuraman, "Complacency and bias in human use of automation: An attentional integration"). Those with more AI experience prioritized Transparency (OR = 1.30), as familiarity with AI's quirks heightened demands for visible reasoning and provenance to "debug and justify outputs even when they look[ed] plausible" (P367). Technophilic individuals prioritized Goal Maintenance (OR = 1.16). As recent work notes (Choudhuri, "What Needs Attention? Prioritizing Drivers of Developers' Trust and Adoption of Generative AI"), their intrinsic drive to explore AI tooling collides with current frictions (prompt churn, drift, and limited affordances), raising the cognitive cost of exploration. Consistent with this pattern, psychological research shows that systems which preserve user intent and minimize orchestration overhead sustain intrinsic engagement (Edward, "The" what" and" why" of goal pursuits: Human needs and the self-determination of behavior")(Unsworth, "Variation in working memory capacity and cognitive control: Goal maintenance and microadjustments of control").
Takeaway: Individual traits shape RAI priorities: SE experience heightens demands for Reliability & Safety, AI experience for Transparency, and technophilia for Goal Maintenance. SE/AI experience, risk tolerance, and technophilia all amplify emphasis on Steerability, reflecting a strong need for agency.

## Discussion

Our results capture a snapshot of a historic inflection point. Findings may shift as tools evolve, yet both current patterns and their theoretical grounding remain informative. Task forms and positions on our map will change as capabilities grow, but deeper structures from appraisal theory (e.g., enduring needs in quality, coding, documentation, coordination, and people work) are likely to persist. Our mixed-method, clustering-based lens is designed for reuse as the landscape evolves, enabling teams to relocate tasks rather than freeze them in time. Current frictions in reliability, security, and transparency highlight where to invest next, especially in the "outer loop" (e.g., testing, review/release, governance).

## Implications for practice

A key implication is to favor augmentation over blunt automation. Developers prefer AI that amplifies creativity and complexity, not just removes toil, consistent with evidence linking meaningful work to growth and contribution rather than extrinsic rewards (Lips-Wiersma, "Why we don't talk about meaning at work"). The "human / AI / human+AI" framing (Unknown, "The New Future of Work") applies: some activities remain human-led, some AI-led, and many are best as human+AI. Our map shows where each mode fits today and where to invest to enhance meaning rather than hollow it out.
Developers want AI as a cognitive collaborator; helping decom-
pose problems, generate alternatives, capture rationale, and pivot across artifacts (code, tests, docs, issues, designs), while preserving oversight, craft, and agency. Concretely: ‚Ä¢ Provenance and transparency: show sources, explanations, confidence, and transformations; keep decision paths inspectable; maintain traceable links among artifacts. ‚Ä¢ Decision control: default to suggest-only flows with reversible changes, batched diffs with rationales, and explicit approval checkpoints. ‚Ä¢ Craft-preserving design: reveal intermediate reasoning and trade-offs so developers learn, avoiding skill erosion from over-automation. Where work depends on connection, negotiation, and recognition, developers de-prioritize AI. The right stance is peripheral support: assist with preparation (briefings, what-if scripts), reflection (summaries, action extraction), and equity (bias checks, inclusivity), while leaving human contact and credit intact. This "complement, don't crowd out" principle mitigates AI intrusion into social labor.
Automation often shifts toil rather than eliminating it (Acemoglu, "Automation and new tasks: How technology displaces and reinstates labor")(Nixon, "Elements of rural economics"). Time saved can reappear in setup, oversight, or remediation. Highest returns pair automation with reliability, transparency, and alignment: strong grounding; guardrails for hallucinations or unsafe edits; testfirst or co-generated tests; and integration-aware suggestions that respect CI, policy, and compliance. Human oversight remains essential as software work is inherently socio-technical and consequential.
RAI priorities vary by task-context and developer disposition; so there is no one-size "copilot". Traits should adapt to the work:
‚Ä¢ Task-aware personas: exploration (diversity over precision), implementation (precision, diff-awareness), review (risk-sensitivity, policy awareness), operations (traceability, rollbacks). ‚Ä¢ User-calibrated agency: adjustable autonomy with clear affordances to ratchet delegation up or down; defaults keyed to task risk.
‚Ä¢ Context diet and guardrails: minimal-necessary context; privacy tiers; least-privilege access; bias and security checks on by default. In practice, ship for augmentation (human-gated control) in Core Work, treat Ops & Coordination as a reliability/traceability problem first, and keep People & AI-Building human-led with AI in a peripheral, assistive role.
Teams can use the map to shift time from low-signal toil to higherorder knowledge and people work, creating room for learning and designing ceremonies that preserve recognition (e.g., crediting rationales and reviews). Leaders should track experience outcomes (flow, satisfaction, confidence) alongside throughput and invest in intentional "moments that matter" to maintain cohesion in an era of AI-accelerated solo work. AI may free time for complex, creative problem-solving and human-facing coordination, but this shift is not automatic; it requires intentional job crafting (redesigning roles, rituals, recognition) so higher-order work is visible/rewarded; and support for horizontal skill expansion (product sense, data/AI literacy, operations). Open questions remain: Does AI truly create time for meaningful human work, or mainly boost throughput? Which jobcrafting moves best preserve meaningful work? Under what conditions?

## Implications for research

As assistants grow more agentic, three priorities emerge:
(1) Transparency & observability. What forms of evidence (e.g., decision logs, artifact lineage, rationales) improve oversight without inducing overreliance (Bu√ßinca, "To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making")? Needed: validated measures of "useful transparency" and experiments on trust calibration/error detection.
(2) Goal maintenance. How should evolving goals and constraints be represented so agents detect and prevent drift across artifacts/sessions? Needed: shared human-& AI-legible goal schemas; drift benchmarks; causal tests of guardrails (pre-commit checks, testfirst prompts) on quality, latency, and friction.
(3) Steerability & developer agency. Which interrupt/redirect mechanisms and delegation policies best balance control under varying risk? Needed: task-typed autonomy ladders/taxonomies (interrupt, revise, roll back); evaluations tying agency to outcome quality.
Because tasks and tools will keep evolving, we present our approach (cognitive appraisal + mixed methods + clustering) as a living instrument. Periodic re-runs (every 6-12 months) can relocate tasks, recalibrate RAI priorities, and test whether improvements in transparency, goal maintenance, and steerability measurably shift developer experience and outcomes.

## Limitations

Construct validity We measured constructs using self-reported items grounded in established theory. Still, surveys can introduce bias or misunderstanding. We mitigated risk by involving practitioners, sandboxing and piloting, randomizing blocks, adding attention checks, and screening patterned responses. To limit burden, we used one item per construct, consistent with evidence that single-item measures remain valid for well-scoped behavioral constructs (Russell A Matthews, "Normalizing the use of single-item measures: Validation of the single-item compendium for organizational psychology").
Internal validity As a cross-sectional study (Stol, "The ABC of software engineering research"), we report associations, not causation. Self-selection bias is possible, since those with stronger views may be more likely to respond. We strengthened validity by triangulating quantitative results with coded qualitative data, reaching team consensus, aligning with theory, and using member checking. As with all survey-based work, results reflect self-reported perceptions. For RAI prioritization, we combined quantitative and qualitative evidence to assess how principles were valued across contexts and groups. Interpret these results with care, since a normative "ought" does not follow from an empirical "is" (Musschenga, "Empirical ethics, context-sensitivity, and contextualism"). That is, a principle is not more or less important simply because respondents (de)prioritized it. Our goal is to inform context-sensitive, RAI choices in SE tooling and to offer critical reflections, not to prescribe one course of action. Following prior recommendations (Norman K Denzin, "The Sage handbook of qualitative research. sage"), we do not report frequencies or percentages for qualitative findings.
External validity We studied Microsoft developers across global sites, diverse teams and roles, many domains, varied processes, and stakeholder contexts. This scope supports industry relevance but may not generalize to smaller organizations or open source communities. We do not claim to represent all software engineers. Instead, we provide an in-depth account of a large and influential organizational context. Single case studies have advanced scientific discovery (Flyvbjerg, "Five misunderstandings about case-study research") and produced insights in social science and software engineering (Kuper, "The social science encyclopedia")(Storey, "Towards a theory of software developer job satisfaction and perceived productivity"). Our findings contribute in this tradition, and future work should test transferability in other contexts.

## Conclusion

Our study shows AI in software engineering should augment, not replace, developers. Demand is highest for tools that cut toil and improve core work, with clear limits around strategic and interpersonal tasks. Developers favor responsible support: reliable and safe, privacy-preserving, transparent, and steerable, so they stay in control and learn. Build goal-aware, observable, interruptible systems, and invest where need outpaces use; putting AI where it matters.
Data availability. Supplementary materials are available at [1]; an interactive dashboard is at https://aka.ms/AI-Where-It-Matters.
